#!/usr/bin/env python3
"""
ä»çœŸå®å®éªŒç»“æœç”Ÿæˆå¯è§†åŒ–æ¼”ç¤ºæ•°æ®
"""
import json
import networkx as nx
import numpy as np
from pathlib import Path
import argparse

def generate_synthetic_graph(results):
    """ä»å®éªŒç»“æœç”Ÿæˆæ¨¡æ‹Ÿå›¾"""
    print("  ğŸ”„ ç”Ÿæˆæ¨¡æ‹Ÿå›¾...")
    stats = results.get('graph_stats', {})
    n_nodes = stats.get('nodes', 50)
    n_edges = stats.get('edges', 200)
    avg_degree = stats.get('avg_degree', 8)
    
    # é™åˆ¶èŠ‚ç‚¹æ•°
    n_nodes = min(n_nodes, 100)
    
    # ä½¿ç”¨BAæ¨¡å‹ç”Ÿæˆæ— æ ‡åº¦ç½‘ç»œ
    m = max(1, int(avg_degree / 2))
    G = nx.barabasi_albert_graph(n_nodes, m, seed=42)
    
    print(f"  âœ… ç”Ÿæˆäº† {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")
    
    # éšæœºåˆ†é…å±æ€§
    for node in G.nodes():
        G.nodes[node]['attribute'] = np.random.choice(['A', 'B', 'C'])
    
    return G

def load_graph(dataset, ego_id=None, results=None):
    """åŠ è½½å›¾æ•°æ®"""
    if dataset == 'facebook_ego':
        if ego_id is None:
            ego_id = '0'
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®è·¯å¾„
        possible_paths = [
            Path('data/datasets/facebook'),
            Path('data/facebook'),
            Path('data'),
            Path('../data/datasets/facebook'),
            Path('../data/facebook'),
            Path('../../data/datasets/facebook'),
        ]
        
        edge_file = None
        feat_file = None
        
        for data_dir in possible_paths:
            edge_candidate = data_dir / f'{ego_id}.edges'
            feat_candidate = data_dir / f'{ego_id}.feat'
            if edge_candidate.exists():
                edge_file = edge_candidate
                feat_file = feat_candidate
                print(f"  ğŸ“ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {edge_file}")
                break
        
        if edge_file is None:
            print(f"  âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°è¾¹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨å®éªŒç»“æœä¸­çš„ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆæ¨¡æ‹Ÿå›¾")
            # ä»å®éªŒç»“æœç”Ÿæˆæ¨¡æ‹Ÿå›¾
            if results:
                return generate_synthetic_graph(results)
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ° {ego_id}.edges æ–‡ä»¶ï¼Œä¹Ÿæ²¡æœ‰æä¾›resultså‚æ•°")
        
        G = nx.Graph()
        
        # è¯»å–è¾¹
        edge_count = 0
        with open(edge_file) as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    try:
                        G.add_edge(int(parts[0]), int(parts[1]))
                        edge_count += 1
                    except ValueError:
                        continue
        
        print(f"  âœ… è¯»å–äº† {edge_count} æ¡è¾¹")
        
        # è¯»å–ç‰¹å¾ï¼ˆä½œä¸ºå±æ€§ï¼‰
        if feat_file and feat_file.exists():
            attr_count = 0
            with open(feat_file) as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        try:
                            node_id = int(parts[0])
                            # ç®€åŒ–ï¼šä½¿ç”¨ç‰¹å¾çš„å‰å‡ ä¸ªç»´åº¦æ¥ç¡®å®šå±æ€§ç±»åˆ«
                            features = [int(x) for x in parts[1:]]
                            attr_sum = sum(features[:3]) if len(features) >= 3 else 0
                            if attr_sum < 1:
                                attr = 'A'
                            elif attr_sum < 2:
                                attr = 'B'
                            else:
                                attr = 'C'
                            if node_id in G.nodes():
                                G.nodes[node_id]['attribute'] = attr
                                attr_count += 1
                        except ValueError:
                            continue
            print(f"  âœ… è¯»å–äº† {attr_count} ä¸ªèŠ‚ç‚¹çš„å±æ€§")
        else:
            print(f"  âš ï¸  ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†éšæœºåˆ†é…å±æ€§")
            # éšæœºåˆ†é…å±æ€§
            for node in G.nodes():
                G.nodes[node]['attribute'] = np.random.choice(['A', 'B', 'C'])
    
    elif dataset == 'cora':
        # åŠ è½½Coraæ•°æ®é›†
        try:
            from torch_geometric.datasets import Planetoid
            import torch_geometric.transforms as T
            
            print(f"  ğŸ“¦ åŠ è½½Coraæ•°æ®é›†...")
            dataset_obj = Planetoid(root='data', name='Cora', transform=T.NormalizeFeatures())
            data = dataset_obj[0]
            
            G = nx.Graph()
            edge_index = data.edge_index.numpy()
            edges = list(zip(edge_index[0], edge_index[1]))
            G.add_edges_from(edges)
            print(f"  âœ… åŠ è½½äº† {len(edges)} æ¡è¾¹")
            
            # æ·»åŠ å±æ€§æ ‡ç­¾
            labels = data.y.numpy()
            attr_count = 0
            for node in G.nodes():
                if node < len(labels):
                    label = int(labels[node])
                    if label == 0:
                        attr = 'A'
                    elif label in [1, 2]:
                        attr = 'B'
                    else:
                        attr = 'C'
                    G.nodes[node]['attribute'] = attr
                    attr_count += 1
            print(f"  âœ… è®¾ç½®äº† {attr_count} ä¸ªèŠ‚ç‚¹çš„å±æ€§")
            
        except ImportError:
            print(f"  âš ï¸  è­¦å‘Š: torch_geometricæœªå®‰è£…ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿå›¾")
            if results:
                return generate_synthetic_graph(results)
            else:
                raise ImportError("éœ€è¦å®‰è£… torch_geometric æˆ–æä¾› results å‚æ•°")
        except Exception as e:
            print(f"  âš ï¸  è­¦å‘Š: åŠ è½½Coraå¤±è´¥ ({e})ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿå›¾")
            if results:
                return generate_synthetic_graph(results)
            else:
                raise
    
    else:
        print(f"  âš ï¸  æœªçŸ¥æ•°æ®é›†: {dataset}ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿå›¾")
        if results:
            return generate_synthetic_graph(results)
        else:
            raise ValueError(f"Unknown dataset: {dataset}")
    
    return G

def compute_layout(G, max_nodes=50):
    """è®¡ç®—å›¾å¸ƒå±€ï¼ˆé™åˆ¶èŠ‚ç‚¹æ•°ä»¥æé«˜æ€§èƒ½ï¼‰"""
    if len(G.nodes()) == 0:
        raise ValueError("å›¾ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—å¸ƒå±€")
    
    # å¦‚æœèŠ‚ç‚¹å¤ªå¤šï¼Œé‡‡æ ·ä¸€ä¸ªå­å›¾
    if len(G.nodes()) > max_nodes:
        # é€‰æ‹©åº¦æ•°æœ€é«˜çš„èŠ‚ç‚¹
        nodes_by_degree = sorted(G.degree(), key=lambda x: x[1], reverse=True)
        selected_nodes = [n for n, d in nodes_by_degree[:max_nodes]]
        G_sub = G.subgraph(selected_nodes).copy()
    else:
        G_sub = G
    
    if len(G_sub.nodes()) == 0:
        raise ValueError("å­å›¾ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—å¸ƒå±€")
    
    # ä½¿ç”¨spring layout
    pos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)
    
    if len(pos) == 0:
        raise ValueError("å¸ƒå±€è®¡ç®—å¤±è´¥ï¼Œæ²¡æœ‰èŠ‚ç‚¹ä½ç½®")
    
    # å½’ä¸€åŒ–åˆ°[0, 600]èŒƒå›´
    pos_array = np.array(list(pos.values()))
    min_pos = pos_array.min(axis=0)
    max_pos = pos_array.max(axis=0)
    
    pos_normalized = {}
    for node, (x, y) in pos.items():
        x_norm = (x - min_pos[0]) / (max_pos[0] - min_pos[0]) * 500 + 50
        y_norm = (y - min_pos[1]) / (max_pos[1] - min_pos[1]) * 500 + 50
        pos_normalized[node] = (x_norm, y_norm)
    
    return pos_normalized, G_sub

def generate_greedy_steps(G, results, max_steps=10):
    """ç”Ÿæˆè´ªå¿ƒåŒ¹é…çš„æ¼”ç¤ºæ­¥éª¤"""
    steps = []
    nodes = list(G.nodes())[:max_steps]
    
    # æ‰¾åˆ°å¯¹åº”çš„ç»“æœ
    greedy_result = None
    for r in results.get('deanonymization', []):
        if 'Greedy' in r['method']:
            greedy_result = r
            break
    
    if greedy_result is None:
        return []
    
    accuracy = greedy_result.get('accuracy', 0.5)
    
    for i, node in enumerate(nodes):
        # æ¨¡æ‹ŸåŒ¹é…è¿‡ç¨‹
        success = np.random.random() < accuracy
        steps.append({
            'orig_node': int(node),
            'anon_node': int(node),  # ç®€åŒ–ï¼šå‡è®¾ç›¸åŒ
            'success': success,
            'similarity': float(np.random.random() * 0.5 + 0.5) if success else float(np.random.random() * 0.5),
            'description': f'åŒ¹é…èŠ‚ç‚¹ {node}: {"æˆåŠŸ" if success else "å¤±è´¥"}'
        })
    
    return steps

def generate_deepwalk_walks(G, n_walks=3, walk_length=5):
    """ç”Ÿæˆéšæœºæ¸¸èµ°æ¼”ç¤º"""
    walks = []
    nodes = list(G.nodes())
    
    for _ in range(n_walks):
        if not nodes:
            break
        start_node = np.random.choice(nodes)
        walk = [int(start_node)]
        current = start_node
        
        for _ in range(walk_length - 1):
            neighbors = list(G.neighbors(current))
            if not neighbors:
                break
            current = np.random.choice(neighbors)
            walk.append(int(current))
        
        walks.append(walk)
    
    return walks

def generate_attribute_inference_steps(G, results, max_steps=8):
    """ç”Ÿæˆå±æ€§æ¨æ–­æ¼”ç¤ºæ­¥éª¤"""
    steps = []
    
    # æ‰¾åˆ°é‚»å±…æŠ•ç¥¨çš„ç»“æœ
    attr_result = None
    for r in results.get('attribute_inference', []):
        if 'Voting' in r['method']:
            attr_result = r
            break
    
    if attr_result is None:
        return []
    
    accuracy = attr_result.get('accuracy', 0.5)
    
    # é€‰æ‹©ä¸€äº›æ²¡æœ‰å±æ€§çš„èŠ‚ç‚¹
    nodes_with_attr = [n for n in G.nodes() if 'attribute' in G.nodes[n]]
    nodes_without_attr = [n for n in G.nodes() if 'attribute' not in G.nodes[n]]
    
    if not nodes_without_attr:
        # å¦‚æœéƒ½æœ‰å±æ€§ï¼Œéšæœºéšè—ä¸€äº›
        nodes_without_attr = np.random.choice(nodes_with_attr, min(max_steps, len(nodes_with_attr)), replace=False)
    
    for node in list(nodes_without_attr)[:max_steps]:
        neighbors = list(G.neighbors(node))
        if not neighbors:
            continue
        
        # ç»Ÿè®¡é‚»å±…å±æ€§
        neighbor_attrs = []
        for n in neighbors:
            if 'attribute' in G.nodes[n]:
                neighbor_attrs.append(G.nodes[n]['attribute'])
        
        if not neighbor_attrs:
            continue
        
        # æŠ•ç¥¨
        from collections import Counter
        votes = Counter(neighbor_attrs)
        predicted = votes.most_common(1)[0][0]
        
        steps.append({
            'node': int(node),
            'neighbors': [int(n) for n in neighbors[:5]],  # æœ€å¤šæ˜¾ç¤º5ä¸ªé‚»å±…
            'votes': dict(votes),
            'predicted': predicted,
            'correct': np.random.random() < accuracy
        })
    
    return steps

def generate_defense_data(G, results):
    """ç”Ÿæˆé˜²å¾¡æ¼”ç¤ºæ•°æ®"""
    defense_result = None
    for r in results.get('defense', []):
        if r.get('epsilon') == 0.1:  # ä½¿ç”¨epsilon=0.1çš„ç»“æœ
            defense_result = r
            break
    
    if defense_result is None:
        return {'edges_to_remove': [], 'edges_to_add': []}
    
    edges = list(G.edges())
    
    # æ ¹æ®structural_lossé€‰æ‹©è¦åˆ é™¤å’Œæ·»åŠ çš„è¾¹
    n_remove = min(10, len(edges) // 10)
    n_add = min(15, len(edges) // 10)
    
    edges_to_remove = list(np.random.choice(len(edges), n_remove, replace=False))
    
    # ç”Ÿæˆè¦æ·»åŠ çš„è¾¹ï¼ˆéšæœºèŠ‚ç‚¹å¯¹ï¼‰
    nodes = list(G.nodes())
    edges_to_add = []
    for _ in range(n_add):
        n1, n2 = np.random.choice(nodes, 2, replace=False)
        if not G.has_edge(n1, n2):
            edges_to_add.append({'source': int(n1), 'target': int(n2)})
    
    return {
        'edges_to_remove': edges_to_remove,
        'edges_to_add': edges_to_add
    }

def graph_to_json(G, pos):
    """å°†å›¾è½¬æ¢ä¸ºJSONæ ¼å¼"""
    nodes = []
    for node in G.nodes():
        if node not in pos:
            continue
        x, y = pos[node]
        nodes.append({
            'id': int(node),
            'index': int(node),
            'x': float(x),
            'y': float(y),
            'degree': G.degree(node),
            'attribute': G.nodes[node].get('attribute', 'Unknown'),
            'known': bool(np.random.random() > 0.3)  # 30%æœªçŸ¥
        })
    
    links = []
    for u, v in G.edges():
        if u in pos and v in pos:
            links.append({
                'source': int(u),
                'target': int(v)
            })
    
    return {'nodes': nodes, 'links': links}

def generate_graph_kernel_data(G):
    """ç”Ÿæˆå›¾æ ¸æ–¹æ³•çš„æ¼”ç¤ºæ•°æ®"""
    nodes = list(G.nodes())
    if not nodes:
        return {}
    
    center_node = np.random.choice(nodes)
    neighbors = list(G.neighbors(center_node))
    
    return {
        'center_node': int(center_node),
        'hops': [
            {'nodes': [int(n) for n in neighbors[:5]]}
        ]
    }

def main():
    parser = argparse.ArgumentParser(description='ä»å®éªŒç»“æœç”Ÿæˆå¯è§†åŒ–æ¼”ç¤ºæ•°æ®')
    parser.add_argument('--result_file', type=str, required=True,
                        help='å®éªŒç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='results/real_demo_data_final.json',
                        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max_nodes', type=int, default=50,
                        help='æœ€å¤§æ˜¾ç¤ºèŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤50ï¼‰')
    
    args = parser.parse_args()
    
    print(f"ğŸ“– è¯»å–å®éªŒç»“æœ: {args.result_file}")
    with open(args.result_file) as f:
        results = json.load(f)
    
    dataset = results['dataset']
    ego_id = results.get('ego_id')
    
    print(f"ğŸ“Š æ•°æ®é›†: {dataset}, Ego ID: {ego_id}")
    print(f"ğŸ“ˆ å›¾ç»Ÿè®¡: {results['graph_stats']}")
    
    # åŠ è½½å›¾
    print("ğŸ”„ åŠ è½½å›¾æ•°æ®...")
    G = load_graph(dataset, ego_id, results)
    print(f"âœ… å›¾åŠ è½½å®Œæˆ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
    
    if G.number_of_nodes() == 0:
        print("âŒ é”™è¯¯: å›¾ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­")
        return
    
    # è®¡ç®—å¸ƒå±€
    print("ğŸ¨ è®¡ç®—å›¾å¸ƒå±€...")
    pos, G_sub = compute_layout(G, max_nodes=args.max_nodes)
    print(f"âœ… ä½¿ç”¨ {len(G_sub.nodes())} ä¸ªèŠ‚ç‚¹è¿›è¡Œå¯è§†åŒ–")
    
    # è½¬æ¢ä¸ºJSONæ ¼å¼
    print("ğŸ”„ ç”Ÿæˆå›¾æ•°æ®...")
    graph_data = graph_to_json(G_sub, pos)
    
    # ç”ŸæˆåŠ¨ç”»æ•°æ®
    print("ğŸ¬ ç”ŸæˆåŠ¨ç”»æ•°æ®...")
    
    print("  - è´ªå¿ƒåŒ¹é…...")
    greedy_steps = generate_greedy_steps(G_sub, results)
    
    print("  - åŒˆç‰™åˆ©ç®—æ³•...")
    hungarian_steps = greedy_steps[:5]  # ä½¿ç”¨å‰5æ­¥ä½œä¸ºç¤ºä¾‹
    
    print("  - å›¾æ ¸æ–¹æ³•...")
    graph_kernel_data = generate_graph_kernel_data(G_sub)
    
    print("  - DeepWalk...")
    deepwalk_walks = generate_deepwalk_walks(G_sub)
    
    print("  - å±æ€§æ¨æ–­...")
    attribute_steps = generate_attribute_inference_steps(G_sub, results)
    
    print("  - é˜²å¾¡æ–¹æ³•...")
    defense_data = generate_defense_data(G_sub, results)
    
    # ç»„è£…æœ€ç»ˆæ•°æ®
    demo_data = {
        'meta': {
            'dataset': dataset,
            'ego_id': ego_id,
            'nodes': len(graph_data['nodes']),
            'edges': len(graph_data['links']),
            'timestamp': results['timestamp']
        },
        'graph': graph_data,
        'results': {
            'deanonymization': results['deanonymization'],
            'attribute_inference': results['attribute_inference'],
            'defense': results['defense']
        },
        'animations': {
            'greedy': greedy_steps,
            'hungarian': hungarian_steps,
            'graph_kernel': graph_kernel_data,
            'deepwalk': {
                'walks': deepwalk_walks
            },
            'attribute_inference': attribute_steps,
            'defense': defense_data
        }
    }
    
    # ä¿å­˜
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {args.output}")
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        else:
            return obj
    
    demo_data = convert_numpy(demo_data)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(demo_data, f, indent=2, ensure_ascii=False)
    
    print("âœ… å®Œæˆï¼")
    print(f"\nğŸ“Š ç”Ÿæˆçš„æ•°æ®ç»Ÿè®¡:")
    print(f"  - èŠ‚ç‚¹æ•°: {len(graph_data['nodes'])}")
    print(f"  - è¾¹æ•°: {len(graph_data['links'])}")
    print(f"  - è´ªå¿ƒæ­¥éª¤: {len(greedy_steps)}")
    print(f"  - éšæœºæ¸¸èµ°: {len(deepwalk_walks)}")
    print(f"  - å±æ€§æ¨æ–­æ­¥éª¤: {len(attribute_steps)}")
    print(f"  - å»åŒ¿ååŒ–æ–¹æ³•: {len(results['deanonymization'])}")
    print(f"  - å±æ€§æ¨æ–­æ–¹æ³•: {len(results['attribute_inference'])}")
    print(f"  - é˜²å¾¡æ–¹æ³•: {len(results['defense'])}")
    
    print(f"\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
    print(f"  1. å°†ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶å¤åˆ¶åˆ°ç½‘é¡µåŒç›®å½•")
    print(f"  2. ä¿®æ”¹ animated_attack_demo.html ä¸­çš„æ•°æ®è·¯å¾„ä¸º: {args.output}")
    print(f"  3. è¿è¡Œ: ./run_animated_demo.sh")

if __name__ == '__main__':
    main()

