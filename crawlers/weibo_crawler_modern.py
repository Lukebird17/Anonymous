"""
ä½¿ç”¨weibo-crawleråº“çš„ç°ä»£åŒ–å¾®åšçˆ¬è™«
è¿™æ˜¯åŸºäº2024å¹´æœ€æ–°æ–¹æ³•çš„æ”¹è¿›ç‰ˆæœ¬
"""

import json
import time
import sys
from pathlib import Path
from tqdm import tqdm
import logging

# è§£å†³æœ¬åœ°æ–‡ä»¶åå†²çªï¼šä¼˜å…ˆä»site-packageså¯¼å…¥
import site
for site_dir in site.getsitepackages():
    if site_dir not in sys.path:
        sys.path.insert(0, site_dir)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¦–å…ˆéœ€è¦å®‰è£…: pip install weibo-crawler
# æ³¨æ„ï¼šåŒ…åæ˜¯weibo-crawlerï¼Œä½†å¯¼å…¥æ—¶ç”¨weibo_crawler
try:
    from weibo_crawler import Profile, Follow
    WEIBO_CRAWLER_AVAILABLE = True
    logger.info("âœ… weibo-crawleråº“å·²åŠ è½½")
except ImportError as e:
    WEIBO_CRAWLER_AVAILABLE = False
    logger.warning(f"weibo-crawleræœªå®‰è£…: {e}")


class ModernWeiboCrawler:
    """ä½¿ç”¨weibo-crawleråº“çš„ç°ä»£åŒ–çˆ¬è™«"""
    
    def __init__(self, cookies: str):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            cookies: å¾®åšcookieså­—ç¬¦ä¸²
        """
        if not WEIBO_CRAWLER_AVAILABLE:
            raise ImportError("è¯·å…ˆå®‰è£…: pip install weibo-crawler")
        
        self.cookies = cookies
        
        # weibo-crawleréœ€è¦æŒ‡å®šCSVæ–‡ä»¶è·¯å¾„
        data_dir = Path(__file__).parent.parent / "data" / "raw"
        data_dir.mkdir(parents=True, exist_ok=True)
        
        profile_csv = str(data_dir / "weibo_profiles.csv")
        follow_csv = str(data_dir / "weibo_follows.csv")
        
        self.profile = Profile(cookies=cookies, csvfile=profile_csv)
        self.follow = Follow(cookies=cookies, csvfile=follow_csv)
    
    def get_user_info(self, uid: str) -> dict:
        """
        è·å–ç”¨æˆ·ä¿¡æ¯
        
        Args:
            uid: ç”¨æˆ·ID
            
        Returns:
            ç”¨æˆ·ä¿¡æ¯å­—å…¸
        """
        try:
            user_data = self.profile.get_profile(userid=uid)
            
            if user_data:
                return {
                    'uid': uid,
                    'screen_name': user_data.get('screen_name', ''),
                    'followers_count': user_data.get('followers_count', 0),
                    'follow_count': user_data.get('follow_count', 0),
                    'description': user_data.get('description', ''),
                }
            else:
                logger.warning(f"æ— æ³•è·å–ç”¨æˆ· {uid} çš„ä¿¡æ¯")
                return None
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ· {uid} ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def get_followings(self, uid: str, max_count: int = 100) -> list:
        """
        è·å–ç”¨æˆ·å…³æ³¨åˆ—è¡¨
        
        Args:
            uid: ç”¨æˆ·ID
            max_count: æœ€å¤§è·å–æ•°é‡
            
        Returns:
            å…³æ³¨çš„ç”¨æˆ·IDåˆ—è¡¨
        """
        try:
            followings = self.follow.get_followings(userid=uid)
            
            if followings and isinstance(followings, list):
                # æå–ç”¨æˆ·ID
                following_uids = []
                for user in followings[:max_count]:
                    if isinstance(user, dict) and 'id' in user:
                        following_uids.append(str(user['id']))
                    elif isinstance(user, str):
                        following_uids.append(user)
                
                return following_uids
            else:
                return []
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ· {uid} å…³æ³¨åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return []
    
    def get_followers(self, uid: str, max_count: int = 100) -> list:
        """
        è·å–ç”¨æˆ·ç²‰ä¸åˆ—è¡¨
        
        Args:
            uid: ç”¨æˆ·ID
            max_count: æœ€å¤§è·å–æ•°é‡
            
        Returns:
            ç²‰ä¸ç”¨æˆ·IDåˆ—è¡¨
        """
        try:
            followers = self.follow.get_followers(userid=uid)
            
            if followers and isinstance(followers, list):
                follower_uids = []
                for user in followers[:max_count]:
                    if isinstance(user, dict) and 'id' in user:
                        follower_uids.append(str(user['id']))
                    elif isinstance(user, str):
                        follower_uids.append(user)
                
                return follower_uids
            else:
                return []
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ· {uid} ç²‰ä¸åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return []
    
    def crawl_network(self, start_uid: str, max_users: int = 1000,
                     max_depth: int = 2, delay: float = 2.0) -> dict:
        """
        BFSçˆ¬å–ç¤¾äº¤ç½‘ç»œ
        
        Args:
            start_uid: èµ·å§‹ç”¨æˆ·ID
            max_users: æœ€å¤§ç”¨æˆ·æ•°
            max_depth: æœ€å¤§æ·±åº¦
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œå»ºè®®>=2ç§’é¿å…è¢«å°
            
        Returns:
            åŒ…å«ç”¨æˆ·ä¿¡æ¯å’Œå…³æ³¨å…³ç³»çš„å­—å…¸
        """
        users = {}
        edges = []
        visited = set()
        queue = [(start_uid, 0)]
        
        pbar = tqdm(total=max_users, desc="çˆ¬å–å¾®åšç”¨æˆ·")
        
        while queue and len(users) < max_users:
            uid, depth = queue.pop(0)
            
            if uid in visited or depth > max_depth:
                continue
            
            visited.add(uid)
            
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = self.get_user_info(uid)
            if not user_info:
                continue
            
            users[uid] = user_info
            pbar.update(1)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
            time.sleep(delay)
            
            # è·å–å…³æ³¨åˆ—è¡¨
            if depth < max_depth:
                followings = self.get_followings(uid, max_count=50)
                
                for following_uid in followings:
                    edges.append((uid, following_uid))
                    
                    if following_uid not in visited and len(users) < max_users:
                        queue.append((following_uid, depth + 1))
                
                # å†æ¬¡å»¶è¿Ÿ
                time.sleep(delay)
        
        pbar.close()
        
        return {
            'users': users,
            'edges': edges,
            'metadata': {
                'start_uid': start_uid,
                'max_depth': max_depth,
                'total_users': len(users),
                'total_edges': len(edges)
            }
        }
    
    def save_data(self, data: dict, output_path: Path):
        """ä¿å­˜æ•°æ®"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")


def test_weibo_crawler():
    """æµ‹è¯•weibo-crawleråº“æ˜¯å¦å¯ç”¨"""
    print("="*60)
    print("æµ‹è¯•weibo-crawleråº“")
    print("="*60)
    
    if not WEIBO_CRAWLER_AVAILABLE:
        print("\nâŒ weibo-crawleråº“æœªå®‰è£…")
        print("\nå®‰è£…æ–¹æ³•:")
        print("  pip install weibo-crawler")
        print("\næˆ–è€…:")
        print("  pip install weibo-crawler -i https://pypi.tuna.tsinghua.edu.cn/simple")
        return False
    
    print("\nâœ… weibo-crawleråº“å·²å®‰è£…")
    print("\nå¯ç”¨çš„ç±»:")
    print("  - Profile: è·å–ç”¨æˆ·ä¿¡æ¯")
    print("  - Follow: è·å–å…³æ³¨/ç²‰ä¸")
    print("  - Weibos: è·å–å¾®åšå†…å®¹")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•åº“æ˜¯å¦å¯ç”¨
    if not test_weibo_crawler():
        print("\n" + "="*60)
        print("ğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ")
        print("="*60)
        print("""
ç”±äºå¾®åšåçˆ¬è™«æœºåˆ¶è¾ƒå¼ºï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆ:

æ–¹æ¡ˆ1: ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆæœ€å¿«ï¼‰
  cd /home/honglianglu/hdd/deanony
  ./run_all.sh

æ–¹æ¡ˆ2: ä½¿ç”¨GitHubæ•°æ®ï¼ˆæ¨èï¼‰
  python step1_generate_data.py  # æ”¹ç”¨GitHub

æ–¹æ¡ˆ3: ä½¿ç”¨å…¬å¼€æ•°æ®é›†
  ä¸‹è½½SNAPçš„Facebook/Twitteræ•°æ®é›†
        """)
        return
    
    # ä½ çš„cookiesï¼ˆéœ€è¦æ›¿æ¢ä¸ºæœ€æ–°çš„ï¼‰
    cookies = "SINAGLOBAL=6740185828856.008.1764257392979; XSRF-TOKEN=Myn4TmTnG35cSjgyYPIJfvmV; SCF=AjiMSHwPp3pk5eVrMx10d6WYKiUi8q5VEC2hifoXmNfxm-mQDE2IPwP4DaI7i_6W3iyQ4sat5D1N02_MdRCywNM.; SUB=_2A25EStb9DeRhGeBP41cR8y3NyDuIHXVnJlY1rDV8PUNbmtANLUbakW9NRTnmMHLzxa3KXAOJoUwYFxbbtUflUmvP; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFUHoUkz0PekjTvoM.HlSOx5JpX5KzhUgL.Foqp1h-7e0epe0M2dJLoIEXLxKBLBo.L12eLxK.LB.-L1K.LxKnL12eLBoqLxKML1K-LB-2LxK.L1K-LB.qt; ALF=02_1769354157; _s_tentry=weibo.com; Apache=7516984194176.165.1766762537792; ULV=1766762537794:2:1:1:7516984194176.165.1766762537792:1764257392981; WBPSESS=2JbmQMfDBf9GhITJyWUUWznL60fHFOFR2V0qqV--Q6QQ6CjSe-HiZ0xa9TFn-LDSQqDlY1BaeyAkFOeWqX_zXuy2IQtbUl_bkq6V5XSWjW4mXeVHy0BlQrpgbFODloUw3x_fxkG6hoMnOUUDzYCjtA=="
    
    start_uid = "6185033137"
    
    print("\nå¼€å§‹çˆ¬å–å¾®åšæ•°æ®...")
    print(f"èµ·å§‹ç”¨æˆ·: {start_uid}")
    print(f"æ³¨æ„: è¯·æ±‚é—´éš”2ç§’ï¼Œé¿å…è¢«å°IP")
    
    try:
        crawler = ModernWeiboCrawler(cookies=cookies)
        
        # å…ˆæµ‹è¯•å•ä¸ªç”¨æˆ·
        print("\næµ‹è¯•è·å–ç”¨æˆ·ä¿¡æ¯...")
        user_info = crawler.get_user_info(start_uid)
        
        if user_info:
            print(f"âœ… ç”¨æˆ·ä¿¡æ¯è·å–æˆåŠŸ:")
            print(f"   æ˜µç§°: {user_info.get('screen_name', 'N/A')}")
            print(f"   ç²‰ä¸æ•°: {user_info.get('followers_count', 0)}")
            print(f"   å…³æ³¨æ•°: {user_info.get('follow_count', 0)}")
            
            # å¼€å§‹å®Œæ•´çˆ¬å–
            print("\nå¼€å§‹çˆ¬å–ç¤¾äº¤ç½‘ç»œ...")
            data = crawler.crawl_network(
                start_uid=start_uid,
                max_users=500,  # å…ˆçˆ¬500ä¸ªæµ‹è¯•
                max_depth=2,
                delay=2.0  # 2ç§’å»¶è¿Ÿ
            )
            
            # ä¿å­˜æ•°æ®
            output_path = Path(__file__).parent.parent / "data" / "raw" / "weibo_data.json"
            crawler.save_data(data, output_path)
            
            print(f"\nâœ… çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š ç”¨æˆ·æ•°: {data['metadata']['total_users']}")
            print(f"ğŸ“Š å…³ç³»æ•°: {data['metadata']['total_edges']}")
            
        else:
            print("\nâŒ æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯")
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. Cookieå·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°è·å–")
            print("2. ç”¨æˆ·IDä¸å­˜åœ¨")
            print("3. è¯¥ç”¨æˆ·è®¾ç½®äº†éšç§ä¿æŠ¤")
            print("\nè·å–æœ€æ–°Cookieçš„æ–¹æ³•:")
            print("1. è®¿é—® https://m.weibo.cn")
            print("2. ç™»å½•ä½ çš„è´¦å·")
            print("3. F12 â†’ Network â†’ åˆ·æ–°é¡µé¢")
            print("4. æ‰¾åˆ°ä»»æ„è¯·æ±‚ â†’ Request Headers â†’ Cookie")
            print("5. å®Œæ•´å¤åˆ¶Cookieå€¼")
            
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("\n" + "="*60)
        print("ğŸ’¡ å»ºè®®")
        print("="*60)
        print("å¾®åšçˆ¬è™«æ¯”è¾ƒå¤æ‚ï¼Œå»ºè®®ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆï¼š")
        print("  ./run_all.sh  # ä½¿ç”¨ç¤ºä¾‹æ•°æ®")


if __name__ == "__main__":
    main()

