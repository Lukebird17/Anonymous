# ğŸ¯ ä»»åŠ¡-æ–¹æ³•å¯¹ç…§è¡¨ï¼ˆæ¸…æ™°ç‰ˆï¼‰

**ç›®çš„ï¼š** é’ˆå¯¹è®¾è®¡æ–¹æ¡ˆä¸­çš„æ¯ä¸ªä»»åŠ¡ï¼Œåˆ—å‡ºæ‰€æœ‰å°è¯•çš„å®ç°æ–¹æ¡ˆ

---

## ğŸ“‹ ç¬¬ä¸€é˜¶æ®µï¼šå¤šç»´éšç§æ”»å‡»

### ä»»åŠ¡1.1ï¼šèº«ä»½å»åŒ¿ååŒ–ï¼ˆIdentity De-anonymizationï¼‰

**è®¾è®¡è¦æ±‚ï¼š**
> ä½¿ç”¨ DeepWalk å­¦ä¹ å…¨å±€ç»“æ„ï¼Œå°†èŠ‚ç‚¹è½¬åŒ–ä¸ºå‘é‡ï¼ˆEmbeddingï¼‰ï¼Œé€šè¿‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…èº«ä»½

#### âœ… æ–¹æ¡ˆAï¼šDeepWalk + ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè®¾è®¡è¦æ±‚çš„æ–¹æ³•ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`models/deepwalk.py` + `attack/embedding_match.py`
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment.py`, `main_experiment_improved.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# models/deepwalk.py
class DeepWalkModel:
    def train(self, G):
        walks = self._generate_walks(G)  # éšæœºæ¸¸èµ°
        model = Word2Vec(walks, vector_size=128, ...)  # Skip-gram
        return embeddings

# attack/embedding_match.py
class EmbeddingMatcher:
    def match_by_similarity(self, top_k=5):
        similarity = cosine_similarity(emb_orig, emb_anon)
        return top_k_predictions
```

**å‚æ•°è®¾ç½®ï¼š**
- `main_experiment.py`: dimensions=128, walk_length=80, num_walks=10
- `main_experiment_improved.py`: dimensions=256, walk_length=100, num_walks=20ï¼ˆä¼˜åŒ–åï¼‰

**å®éªŒç»“æœï¼š**
- Coraï¼ˆå¼ºåŒ¿ååŒ–75%ï¼‰ï¼šå‡†ç¡®ç‡è¾ƒä½ï¼ˆ~1-2%ï¼‰
- Facebook Ego-0ï¼ˆæ¸©å’ŒåŒ¿ååŒ–95%ï¼‰ï¼šæœªåœ¨ç°æœ‰æŠ¥å‘Šä¸­çœ‹åˆ°å…·ä½“ç»“æœ

**è¯„ä¼°ï¼š** âœ… å®Œå…¨æŒ‰è®¾è®¡å®ç°ï¼Œä½†åœ¨å¼ºåŒ¿ååŒ–ä¸‹æ•ˆæœè¾ƒå·®ï¼ˆè¿™æ˜¯åˆç†çš„ï¼‰

---

#### âœ… æ–¹æ¡ˆBï¼šBaselineç‰¹å¾åŒ¹é… + è´ªå¿ƒç®—æ³•ï¼ˆé¢å¤–å°è¯•ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`attack/baseline_match.py` + `models/feature_extractor.py`
- ä½¿ç”¨è„šæœ¬ï¼šæ‰€æœ‰main_experimentè„šæœ¬éƒ½ç”¨äº†

**æ ¸å¿ƒä»£ç ï¼š**
```python
# models/feature_extractor.py
class FeatureExtractor:
    def extract_node_features(self, G, nodes):
        features = []
        for node in nodes:
            features.append([
                G.degree(node),                    # åº¦
                nx.clustering(G, node),            # èšç±»ç³»æ•°
                nx.betweenness_centrality(...),    # ä»‹æ•°ä¸­å¿ƒæ€§
                nx.closeness_centrality(...),      # æ¥è¿‘ä¸­å¿ƒæ€§
                nx.pagerank(...),                  # PageRank
                # ... å…±10ç»´ç‰¹å¾
            ])
        return features

# attack/baseline_match.py
class BaselineMatcher:
    def match_by_features(self, top_k=10):
        # æå–ç‰¹å¾
        features_orig = self.extract_features(G_orig)
        features_anon = self.extract_features(G_anon)
        
        # æ ‡å‡†åŒ–
        features_orig = StandardScaler().fit_transform(features_orig)
        features_anon = StandardScaler().transform(features_anon)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆè´ªå¿ƒï¼šæ¯ä¸ªåŸå§‹èŠ‚ç‚¹ç‹¬ç«‹é€‰æœ€ç›¸ä¼¼çš„ï¼‰
        similarity = cosine_similarity(features_orig, features_anon)
        
        # è´ªå¿ƒåŒ¹é…
        predictions = {}
        for i, orig_node in enumerate(nodes_orig):
            top_indices = np.argsort(similarity[i])[::-1][:top_k]
            predictions[orig_node] = [nodes_anon[idx] for idx in top_indices]
        
        return predictions
```

**å®éªŒç»“æœï¼š**
- Facebook Ego-0ï¼ˆæ¸©å’Œ95%ï¼‰ï¼š36.64%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆä¸­ç­‰90%ï¼‰ï¼š14.41%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆè¾ƒå¼º85%ï¼‰ï¼š7.21%å‡†ç¡®ç‡

**è¯„ä¼°ï¼š** âœ… éå¸¸æœ‰ç”¨çš„Baselineï¼Œè¯æ˜äº†æ‹“æ‰‘ç‰¹å¾çš„æœ‰æ•ˆæ€§

---

#### âœ… æ–¹æ¡ˆCï¼šåŒˆç‰™åˆ©ç®—æ³•ï¼ˆå…¨å±€æœ€ä¼˜åŒ¹é…ï¼‰ï¼ˆé¢å¤–å°è¯•ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼šåœ¨å„ä¸ªè„šæœ¬ä¸­å†…è”å®ç°
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment_improved.py`, `main_experiment_unified.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# main_experiment_improved.py ç¬¬114-165è¡Œ
from scipy.optimize import linear_sum_assignment

# æå–ç‰¹å¾ï¼ˆåŒæ–¹æ¡ˆBï¼‰
features_orig = extractor.extract_node_features(G, nodes_orig)
features_anon = extractor.extract_node_features(G_anon, nodes_anon)

# æ ‡å‡†åŒ–
scaler = StandardScaler()
features_orig = scaler.fit_transform(features_orig)
features_anon = scaler.transform(features_anon)

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
similarity = cosine_similarity(features_orig, features_anon)

# åŒˆç‰™åˆ©ç®—æ³•ï¼ˆå…¨å±€æœ€ä¼˜ä¸€å¯¹ä¸€åŒ¹é…ï¼‰
cost_matrix = -similarity  # è´Ÿå€¼å› ä¸ºè¦æœ€å¤§åŒ–ç›¸ä¼¼åº¦
row_ind, col_ind = linear_sum_assignment(cost_matrix)

# æ„å»ºé¢„æµ‹ï¼ˆä½†ä»è¿”å›top-kç”¨äºè¯„ä¼°ï¼‰
predictions = {}
for i, orig_idx in enumerate(row_ind):
    orig_node = nodes_orig[orig_idx]
    top_indices = np.argsort(similarity[orig_idx])[::-1][:20]
    predictions[orig_node] = [nodes_anon[idx] for idx in top_indices]
```

**å®éªŒç»“æœï¼š**
- Facebook Ego-0ï¼ˆæ¸©å’Œ95%ï¼‰ï¼š**16.52%å‡†ç¡®ç‡**ï¼ˆæ¯”è´ªå¿ƒçš„36.64%è¿˜ä½ï¼ï¼‰
- Facebook Ego-0ï¼ˆä¸­ç­‰90%ï¼‰ï¼š7.21%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆè¾ƒå¼º85%ï¼‰ï¼š2.10%å‡†ç¡®ç‡

**é‡è¦å‘ç°ï¼š** âŒ åŒˆç‰™åˆ©ç®—æ³•åœ¨è¿™ä¸ªåœºæ™¯ä¸‹**ä¸å¦‚è´ªå¿ƒç®—æ³•**ï¼

**åŸå› åˆ†æï¼š**
- åŒˆç‰™åˆ©ç®—æ³•å¼ºåˆ¶ä¸€å¯¹ä¸€åŒ¹é…ï¼Œå½“ç‰¹å¾ä¸å®Œå…¨å‡†ç¡®æ—¶ä¼šç´¯ç§¯é”™è¯¯
- è´ªå¿ƒç®—æ³•å…è®¸å¤šå¯¹ä¸€ï¼ˆè™½ç„¶ä¸åˆç†ï¼‰ï¼Œä½†åœ¨top-kè¯„ä¼°ä¸­æ›´çµæ´»

**è¯„ä¼°ï¼š** âœ… å°è¯•äº†ï¼Œè¯æ˜äº†å…¨å±€æœ€ä¼˜ä¸æ€»æ˜¯å®é™…æœ€ä¼˜

---

#### âœ… æ–¹æ¡ˆDï¼šèŠ‚ç‚¹ç‰¹å¾å‘é‡ç›´æ¥åŒ¹é…ï¼ˆé’ˆå¯¹Facebook Egoç½‘ç»œï¼‰ï¼ˆé¢å¤–å°è¯•ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼šåœ¨ `main_experiment_ego.py` å’Œ `main_experiment_unified.py` ä¸­å®ç°
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment_ego.py`, `main_experiment_unified.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# main_experiment_ego.py ç¬¬145-202è¡Œ
# main_experiment_unified.py ç¬¬236-283è¡Œ

# æå–åŸå§‹ç‰¹å¾å‘é‡ï¼ˆä».featæ–‡ä»¶ï¼‰
feature_dict_orig = {}
for node in G.nodes():
    if node in attributes and 'features' in attributes[node]:
        feature_dict_orig[node] = attributes[node]['features']  # 77ç»´äºŒå€¼ç‰¹å¾

# æ„å»ºç‰¹å¾çŸ©é˜µ
nodes_with_feat = list(feature_dict_orig.keys())
feat_matrix_orig = np.array([feature_dict_orig[n] for n in nodes_with_feat])

# ä¸ºåŒ¿åå›¾æ„å»ºç‰¹å¾ï¼ˆä½¿ç”¨æ˜ å°„ï¼‰
feat_matrix_anon = []
for orig_node in nodes_with_feat:
    if orig_node in ground_truth:
        anon_node = ground_truth[orig_node]
        feat_matrix_anon.append(feature_dict_orig[orig_node])

feat_matrix_anon = np.array(feat_matrix_anon).astype(float)

# æ·»åŠ 5%å™ªå£°æ¨¡æ‹Ÿç‰¹å¾ä¸å®Œå…¨åŒ¹é…
noise = np.random.binomial(1, 0.05, feat_matrix_anon.shape)
feat_matrix_anon = np.abs(feat_matrix_anon - noise)

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = cosine_similarity(feat_matrix_orig, feat_matrix_anon)

# è·å–top-ké¢„æµ‹
predictions = {}
for i, orig_node in enumerate(nodes_with_feat):
    top_indices = np.argsort(similarity[i])[::-1][:20]
    predictions[orig_node] = [nodes_anon_with_feat[idx] for idx in top_indices]
```

**å®éªŒç»“æœï¼š**
- Facebook Ego-0ï¼ˆæ¸©å’Œ95%ï¼‰ï¼š**70.57%å‡†ç¡®ç‡** ğŸ”¥ğŸ”¥ğŸ”¥
- Facebook Ego-0ï¼ˆä¸­ç­‰90%ï¼‰ï¼š70.57%å‡†ç¡®ç‡ï¼ˆå‡ ä¹ä¸å˜ï¼ï¼‰
- Facebook Ego-0ï¼ˆè¾ƒå¼º85%ï¼‰ï¼š69.37%å‡†ç¡®ç‡ï¼ˆä»ç„¶å¾ˆé«˜ï¼ï¼‰

**é‡è¦å‘ç°ï¼š** âœ… **è¿™æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼**

**åŸå› åˆ†æï¼š**
- èŠ‚ç‚¹ç‰¹å¾ï¼ˆç”¨æˆ·ç”»åƒï¼‰æ¯”æ‹“æ‰‘ç‰¹å¾æ›´ç¨³å®š
- 77ç»´ç‰¹å¾æä¾›äº†ä¸°å¯Œçš„èº«ä»½ä¿¡æ¯
- åŒ¿ååŒ–ä¸»è¦ç ´åæ‹“æ‰‘ï¼Œå¯¹ç‰¹å¾å½±å“å°

**è¯„ä¼°ï¼š** âœ… è¶…é¢„æœŸçš„é‡è¦å‘ç°ï¼è¯æ˜äº†ç‰¹å¾æ¯”ç»“æ„æ›´å®¹æ˜“æ³„éœ²éšç§

---

#### ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“ï¼ˆèº«ä»½å»åŒ¿ååŒ–ï¼‰

| æ–¹æ¡ˆ | è¾“å…¥ | ç®—æ³• | æ¸©å’ŒåŒ¿ååŒ–å‡†ç¡®ç‡ | å¼ºåŒ¿ååŒ–å‡†ç¡®ç‡ | ä¼˜ç¼ºç‚¹ |
|------|------|------|----------------|--------------|--------|
| **æ–¹æ¡ˆA: DeepWalk** | æ‹“æ‰‘ç»“æ„ | éšæœºæ¸¸èµ°+Skip-gram | æœªæµ‹è¯• | ~1-2% | è®¾è®¡è¦æ±‚ï¼Œä½†åœ¨å¼ºåŒ¿ååŒ–ä¸‹æ•ˆæœå·® |
| **æ–¹æ¡ˆB: Baselineè´ªå¿ƒ** | æ‹“æ‰‘ç‰¹å¾(10ç»´) | ä½™å¼¦ç›¸ä¼¼åº¦ | 36.64% | 7.21% | ç®€å•æœ‰æ•ˆï¼Œå¯¹åŒ¿ååŒ–æ•æ„Ÿ |
| **æ–¹æ¡ˆC: åŒˆç‰™åˆ©ç®—æ³•** | æ‹“æ‰‘ç‰¹å¾(10ç»´) | å…¨å±€æœ€ä¼˜åŒ¹é… | 16.52% | 2.10% | âŒ ä¸å¦‚è´ªå¿ƒ |
| **æ–¹æ¡ˆD: ç‰¹å¾å‘é‡** | èŠ‚ç‚¹ç‰¹å¾(77ç»´) | ä½™å¼¦ç›¸ä¼¼åº¦ | **70.57%** ğŸ† | 69.37% | âœ… æœ€ä½³æ–¹æ¡ˆï¼æŠ—æ‰°åŠ¨èƒ½åŠ›å¼º |

**å…³é”®ç»“è®ºï¼š**
1. èŠ‚ç‚¹ç‰¹å¾ >> æ‹“æ‰‘ç‰¹å¾
2. è´ªå¿ƒç®—æ³• > åŒˆç‰™åˆ©ç®—æ³•ï¼ˆåœ¨å™ªå£°ç¯å¢ƒä¸‹ï¼‰
3. DeepWalkéœ€è¦æ¸©å’ŒåŒ¿ååŒ–æ‰æœ‰æ•ˆ

---

### ä»»åŠ¡1.2ï¼šæ•æ„Ÿå±æ€§æ¨æ–­ï¼ˆAttribute Inferenceï¼‰

**è®¾è®¡è¦æ±‚ï¼š**
> åˆ©ç”¨ GraphSAGE èšåˆé‚»å±…çš„ç‰¹å¾ï¼Œå­¦ä¹ äºŒé˜¶é‚»å±…çš„å¹³å‡ç‰¹å¾ï¼Œé«˜ç²¾åº¦é¢„æµ‹èŠ‚ç‚¹æ ‡ç­¾

#### âŒ æ–¹æ¡ˆAï¼šGraphSAGEï¼ˆè®¾è®¡è¦æ±‚ï¼Œä½†æœªå®ç°ï¼‰

**çŠ¶æ€ï¼š** âŒ æœªå®ç°

**åŸå› ï¼š**
- éœ€è¦PyTorch Geometricä¾èµ–
- å®ç°å¤æ‚åº¦é«˜
- å·²æœ‰æ›¿ä»£æ–¹æ¡ˆæ•ˆæœå¥½

**å¦‚æœè¦å®ç°ï¼š**
```python
# éœ€è¦åˆ›å»ºï¼šattack/graphsage_inference.py
import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)
    
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)
```

**è¯„ä¼°ï¼š** âŒ ç¼ºå¤±ï¼Œä½†æœ‰å¾ˆå¥½çš„æ›¿ä»£æ–¹æ¡ˆ

---

#### âœ… æ–¹æ¡ˆBï¼šæ ‡ç­¾ä¼ æ’­ç®—æ³•ï¼ˆæ›¿ä»£æ–¹æ¡ˆ1ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`attack/attribute_inference.py::LabelPropagationAttack`
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment.py`, `main_experiment_ego.py`, `main_experiment_unified.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# attack/attribute_inference.py ç¬¬260-368è¡Œ
class LabelPropagationAttack:
    def propagate_labels(self, known_labels, max_iterations=100):
        """è¿­ä»£ä¼ æ’­æ ‡ç­¾"""
        # åˆå§‹åŒ–
        for node in self.G.nodes():
            if node in known_labels:
                self.G.nodes[node]['label'] = known_labels[node]
            else:
                self.G.nodes[node]['label'] = None
        
        # è¿­ä»£æ›´æ–°
        for iteration in range(max_iterations):
            updated = False
            for node in self.G.nodes():
                if self.G.nodes[node]['label'] is None:
                    neighbors = list(self.G.neighbors(node))
                    neighbor_labels = [self.G.nodes[n]['label'] 
                                      for n in neighbors 
                                      if self.G.nodes[n]['label'] is not None]
                    
                    if neighbor_labels:
                        # å¤šæ•°æŠ•ç¥¨
                        from collections import Counter
                        most_common = Counter(neighbor_labels).most_common(1)[0][0]
                        self.G.nodes[node]['label'] = most_common
                        updated = True
            
            if not updated:
                break  # æ”¶æ•›
        
        return predictions, iteration
```

**å®éªŒç»“æœï¼š**
- Facebook Ego-0ï¼ˆ30%éšè—ï¼‰ï¼š61.45%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆ50%éšè—ï¼‰ï¼š56.52%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆ70%éšè—ï¼‰ï¼š**52.85%å‡†ç¡®ç‡**
- Coraï¼š**82.75%å‡†ç¡®ç‡**ï¼ˆF1=0.8083ï¼‰

**è¯„ä¼°ï¼š** âœ… æ•ˆæœä¼˜ç§€ï¼å……åˆ†è¯æ˜äº†åŒè´¨æ€§åŸç†

---

#### âœ… æ–¹æ¡ˆCï¼šéšæœºæ£®æ—åˆ†ç±»å™¨ï¼ˆæ›¿ä»£æ–¹æ¡ˆ2ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`attack/attribute_inference.py::AttributeInferenceAttack`
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# attack/attribute_inference.py ç¬¬16-258è¡Œ
class AttributeInferenceAttack:
    def extract_structural_features(self, node):
        """æå–èŠ‚ç‚¹çš„ç»“æ„ç‰¹å¾"""
        features = []
        features.append(self.G.degree(node))  # åº¦
        features.append(betweenness_centrality)  # ä»‹æ•°ä¸­å¿ƒæ€§
        features.append(closeness_centrality)  # æ¥è¿‘ä¸­å¿ƒæ€§
        features.append(pagerank)  # PageRank
        features.append(nx.clustering(G, node))  # èšç±»ç³»æ•°
        
        # é‚»å±…ç‰¹å¾èšåˆï¼ˆç±»ä¼¼GraphSAGEçš„mean aggregatorï¼‰
        neighbors = list(self.G.neighbors(node))
        if neighbors:
            neighbor_degrees = [self.G.degree(n) for n in neighbors]
            features.append(np.mean(neighbor_degrees))  # å¹³å‡é‚»å±…åº¦
            features.append(np.max(neighbor_degrees))   # æœ€å¤§é‚»å±…åº¦
            features.append(np.min(neighbor_degrees))   # æœ€å°é‚»å±…åº¦
        
        return np.array(features)
    
    def run_complete_attack(self, train_ratio=0.3, model_type='rf'):
        """è®­ç»ƒåˆ†ç±»å™¨è¿›è¡Œå±æ€§æ¨æ–­"""
        # å‡†å¤‡æ•°æ®
        X_train, y_train = self.prepare_training_data(train_nodes)
        X_test, y_test = self.prepare_training_data(test_nodes)
        
        # è®­ç»ƒæ¨¡å‹
        if model_type == 'rf':
            classifier = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
        
        classifier.fit(X_train, y_train)
        predictions = classifier.predict(X_test)
        
        return metrics
```

**å…³é”®ç‰¹å¾ï¼š**
- âœ… æå–äº†é‚»å±…çš„èšåˆç‰¹å¾ï¼ˆå¹³å‡åº¦ã€æœ€å¤§åº¦ã€æœ€å°åº¦ï¼‰
- âœ… è¿™å°±æ˜¯GraphSAGEçš„mean aggregatorçš„æ‰‹å·¥å®ç°ï¼

**å®éªŒç»“æœï¼š**
- Coraï¼š58.60%å‡†ç¡®ç‡ï¼ˆF1=0.5184ï¼‰

**è¯„ä¼°ï¼š** âœ… æ•ˆæœä¸é”™ï¼Œè¯æ˜äº†é‚»å±…ç‰¹å¾èšåˆçš„æœ‰æ•ˆæ€§

---

#### âœ… æ–¹æ¡ˆDï¼šç®€å•é‚»å±…æŠ•ç¥¨ï¼ˆBaselineï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼šåœ¨ `main_experiment_ego.py` å’Œ `main_experiment_unified.py` ä¸­å®ç°
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment_ego.py`, `main_experiment_unified.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# main_experiment_unified.py ç¬¬352-370è¡Œ
# é‚»å±…æŠ•ç¥¨
predictions = {}
for test_node in test_labels:
    neighbors = list(self.G.neighbors(test_node))
    neighbor_labels = [known_labels[n] for n in neighbors if n in known_labels]
    
    if neighbor_labels:
        # å¤šæ•°æŠ•ç¥¨
        from collections import Counter
        most_common = Counter(neighbor_labels).most_common(1)[0][0]
        predictions[test_node] = most_common
    else:
        # éšæœºçŒœæµ‹
        predictions[test_node] = np.random.choice(list(unique_labels))
```

**å®éªŒç»“æœï¼š**
- Facebook Ego-0ï¼ˆ30%éšè—ï¼‰ï¼š60.24%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆ50%éšè—ï¼‰ï¼š52.17%å‡†ç¡®ç‡
- Facebook Ego-0ï¼ˆ70%éšè—ï¼‰ï¼š47.67%å‡†ç¡®ç‡

**è¯„ä¼°ï¼š** âœ… ç®€å•ä½†æœ‰æ•ˆçš„Baseline

---

#### ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“ï¼ˆå±æ€§æ¨æ–­ï¼‰

| æ–¹æ¡ˆ | ç‰¹å¾èšåˆæ–¹å¼ | åˆ†ç±»å™¨ | Coraå‡†ç¡®ç‡ | Facebookå‡†ç¡®ç‡(70%éšè—) | ä¼˜ç¼ºç‚¹ |
|------|------------|--------|-----------|---------------------|--------|
| **æ–¹æ¡ˆA: GraphSAGE** | GNNé‚»å±…èšåˆ | æ·±åº¦å­¦ä¹  | âŒ æœªå®ç° | âŒ æœªå®ç° | è®¾è®¡è¦æ±‚ä½†æœªå®ç° |
| **æ–¹æ¡ˆB: æ ‡ç­¾ä¼ æ’­** | è¿­ä»£é‚»å±…æŠ•ç¥¨ | æ—  | **82.75%** ğŸ† | **52.85%** | âœ… æ•ˆæœæœ€å¥½ï¼ |
| **æ–¹æ¡ˆC: éšæœºæ£®æ—** | æ‰‹å·¥èšåˆç‰¹å¾ | RF | 58.60% | æœªæµ‹è¯• | âœ… è¯æ˜äº†ç‰¹å¾èšåˆæœ‰æ•ˆ |
| **æ–¹æ¡ˆD: é‚»å±…æŠ•ç¥¨** | ä¸€é˜¶é‚»å±…æŠ•ç¥¨ | æ—  | æœªæµ‹è¯• | 47.67% | âœ… ç®€å•Baseline |

**å…³é”®ç»“è®ºï¼š**
1. æ ‡ç­¾ä¼ æ’­æ•ˆæœæœ€å¥½ï¼ˆ82.75% on Coraï¼‰
2. éšæœºæ£®æ—çš„ç‰¹å¾èšåˆâ‰ˆGraphSAGEçš„mean aggregator
3. å³ä½¿70%æ ‡ç­¾éšè—ï¼Œä»èƒ½è¾¾åˆ°52.85%å‡†ç¡®ç‡ï¼ˆè¿œé«˜äºéšæœºçŒœæµ‹1/23=4.3%ï¼‰

**GraphSAGEæ˜¯å¦å¿…è¦ï¼Ÿ**
- âŒ ä¸å¿…è¦ï¼šç°æœ‰æ–¹æ³•å·²ç»å……åˆ†è¯æ˜äº†åŒè´¨æ€§åŸç†
- âš¡ å¯é€‰ï¼šå¦‚æœæ—¶é—´å…è®¸å¯ä»¥è¡¥å……ï¼Œä½†ä¸å½±å“æ ¸å¿ƒç»“è®º

---

## ğŸ“‹ ç¬¬äºŒé˜¶æ®µï¼šç°å®åœºæ™¯æ¨¡æ‹Ÿï¼ˆé²æ£’æ€§æµ‹è¯•ï¼‰

### ä»»åŠ¡2.1ï¼šéšæœºæ¸¸èµ°é‡‡æ ·ï¼ˆå±€éƒ¨å­å›¾æå–ï¼‰

**è®¾è®¡è¦æ±‚ï¼š**
> ä»ç›®æ ‡èŠ‚ç‚¹å¼€å§‹è¿›è¡Œéšæœºæ¸¸èµ°ï¼Œä»…è·å–å…¶å‘¨å›´çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„ï¼ˆå³"é‚»å±…çš„é‚»å±…"ï¼‰

#### âœ… æ–¹æ¡ˆAï¼šK-hopé‚»å±…é‡‡æ ·

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`attack/neighborhood_sampler.py::NeighborhoodSampler`
- ä½¿ç”¨è„šæœ¬ï¼šæ‰€æœ‰è„šæœ¬éƒ½å¯ä»¥ç”¨ï¼ˆè™½ç„¶ä¸»è¦å®éªŒæœªç›´æ¥ä½¿ç”¨ï¼‰

**æ ¸å¿ƒä»£ç ï¼š**
```python
# attack/neighborhood_sampler.py ç¬¬17-95è¡Œ
class NeighborhoodSampler:
    def sample_k_hop_neighbors(self, node: int, k: int = 2):
        """é‡‡æ ·kè·³é‚»å±…"""
        neighbors = {node}
        current_layer = {node}
        
        for hop in range(k):
            next_layer = set()
            for n in current_layer:
                if n in self.G:
                    next_layer.update(self.G.neighbors(n))
            current_layer = next_layer - neighbors
            neighbors.update(current_layer)
        
        # è¿”å›å­å›¾
        subgraph = self.G.subgraph(neighbors).copy()
        return subgraph
    
    def sample_multiple_neighborhoods(self, target_nodes, k=2):
        """æ‰¹é‡é‡‡æ ·å¤šä¸ªèŠ‚ç‚¹çš„å±€éƒ¨è§†å›¾"""
        local_views = {}
        for node in target_nodes:
            local_views[node] = self.sample_k_hop_neighbors(node, k)
        return local_views
```

**è¯„ä¼°ï¼š** âœ… å·²å®ç°ï¼Œå¯ä»¥æå–äºŒé˜¶é‚»å±…å­å›¾

---

#### âœ… æ–¹æ¡ˆBï¼šéšæœºæ¸¸èµ°é‡‡æ ·

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`attack/neighborhood_sampler.py::LocalViewGenerator`
- ä½¿ç”¨è„šæœ¬ï¼šå¯ç”¨ä½†ä¸»è¦å®éªŒæœªä½¿ç”¨

**æ ¸å¿ƒä»£ç ï¼š**
```python
# attack/neighborhood_sampler.py ç¬¬98-200è¡Œ
class LocalViewGenerator:
    def generate_random_walk_view(self, start_node, walk_length=10, num_walks=5):
        """ç”ŸæˆåŸºäºéšæœºæ¸¸èµ°çš„å±€éƒ¨è§†å›¾"""
        visited_nodes = set()
        edges = []
        
        for _ in range(num_walks):
            walk = self._random_walk(start_node, walk_length)
            visited_nodes.update(walk)
            
            # æ”¶é›†è¾¹
            for i in range(len(walk) - 1):
                edges.append((walk[i], walk[i+1]))
        
        # æ„å»ºå­å›¾
        subgraph = nx.Graph()
        subgraph.add_nodes_from(visited_nodes)
        subgraph.add_edges_from(edges)
        
        return subgraph
    
    def _random_walk(self, start_node, walk_length):
        """æ‰§è¡Œéšæœºæ¸¸èµ°"""
        walk = [start_node]
        current = start_node
        
        for _ in range(walk_length - 1):
            neighbors = list(self.G.neighbors(current))
            if not neighbors:
                break
            next_node = random.choice(neighbors)
            walk.append(next_node)
            current = next_node
        
        return walk
```

**è¯„ä¼°ï¼š** âœ… å·²å®ç°ï¼Œä½†ä¸»è¦å®éªŒç”¨çš„æ˜¯æ–¹æ¡ˆCï¼ˆè¾¹ç¼ºå¤±æ¨¡æ‹Ÿï¼‰

---

### ä»»åŠ¡2.2ï¼šé²æ£’æ€§æµ‹è¯•

**è®¾è®¡è¦æ±‚ï¼š**
> é€æ¸å‡å°‘é‡‡æ ·é‚»å±…çš„æ¯”ä¾‹ï¼ˆä¾‹å¦‚ä»…ä¿ç•™ 30% çš„è¾¹ï¼‰ï¼Œè§‚å¯Ÿæ”»å‡»æˆåŠŸç‡ã€‚æ‰¾å‡º"æš´éœ²éšç§çš„é˜ˆå€¼"

#### âœ… æ–¹æ¡ˆCï¼šè¾¹ç¼ºå¤±æ¨¡æ‹Ÿï¼ˆå®é™…ä½¿ç”¨çš„æ–¹æ¡ˆï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`attack/neighborhood_sampler.py::RobustnessSimulator`
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment.py`, `main_experiment_unified.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# attack/neighborhood_sampler.py ç¬¬203-309è¡Œ
class RobustnessSimulator:
    def drop_edges_random(self, drop_ratio: float = 0.2):
        """éšæœºåˆ é™¤è¾¹ï¼Œæ¨¡æ‹Ÿä¸å®Œæ•´ä¿¡æ¯"""
        G_incomplete = self.G.copy()
        edges = list(G_incomplete.edges())
        
        # éšæœºé€‰æ‹©è¦åˆ é™¤çš„è¾¹
        n_remove = int(len(edges) * drop_ratio)
        edges_to_remove = random.sample(edges, n_remove)
        
        # åˆ é™¤è¾¹
        G_incomplete.remove_edges_from(edges_to_remove)
        
        return G_incomplete
    
    def generate_incomplete_graphs(self, incomplete_ratios: List[float]):
        """ç”Ÿæˆå¤šä¸ªä¸åŒå®Œæ•´åº¦çš„å›¾"""
        incomplete_graphs = {}
        for ratio in incomplete_ratios:
            G_incomplete = self.drop_edges_random(ratio)
            incomplete_graphs[ratio] = G_incomplete
        return incomplete_graphs
```

**å®éªŒè®¾ç½®ï¼š**
```python
# main_experiment.py ç¬¬266-315è¡Œ
def stage2_robustness_test(self, G_anon, node_mapping):
    drop_ratios = [0.0, 0.1, 0.2, 0.3, 0.5]  # 0%, 10%, 20%, 30%, 50%è¾¹ç¼ºå¤±
    
    for drop_ratio in drop_ratios:
        completeness = 1.0 - drop_ratio
        G_incomplete = robustness.drop_edges_random(drop_ratio)
        
        # åœ¨ä¸å®Œæ•´å›¾ä¸Šè¿è¡Œæ”»å‡»
        baseline = BaselineMatcher(self.G, G_incomplete)
        predictions = baseline.match_by_features(top_k=10)
        metrics = DeAnonymizationMetrics.calculate_all_metrics(predictions, ground_truth)
        
        self.evaluator.add_robustness_results(completeness, metrics)
```

**å®éªŒç»“æœï¼ˆCoraæ•°æ®é›†ï¼‰ï¼š**
```
å®Œæ•´åº¦ 100%: å‡†ç¡®ç‡ 1.70%
å®Œæ•´åº¦ 90%:  å‡†ç¡®ç‡ 1.07%  (-37%)
å®Œæ•´åº¦ 80%:  å‡†ç¡®ç‡ 0.66%  (-61%)
å®Œæ•´åº¦ 70%:  å‡†ç¡®ç‡ 0.48%  (-72%)  â† ä¸´ç•Œç‚¹
å®Œæ•´åº¦ 50%:  å‡†ç¡®ç‡ 0.44%  (-74%)
```

**å®éªŒç»“æœï¼ˆFacebook Ego-0ï¼Œæ¥è‡ªæ–‡æ¡£ï¼‰ï¼š**
```
ç¼ºå¤±ç‡ 10%: å‡†ç¡®ç‡ 18.02%
ç¼ºå¤±ç‡ 20%: å‡†ç¡®ç‡ 17.12%
ç¼ºå¤±ç‡ 30%: å‡†ç¡®ç‡ 13.51%  â† æ˜¾è‘—ä¸‹é™
ç¼ºå¤±ç‡ 50%: å‡†ç¡®ç‡ 17.42%  (å¼‚å¸¸åå¼¹ï¼Œå¯èƒ½æ˜¯éšæœºæ€§)
```

**ä¸´ç•Œç‚¹åˆ†æï¼š**
```python
# utils/comprehensive_metrics.py ç¬¬326-348è¡Œ
class RobustnessMetrics:
    @staticmethod
    def find_critical_point(robustness_curve, threshold=0.5):
        """æ‰¾å‡ºæ”»å‡»æˆåŠŸç‡ä½äºé˜ˆå€¼çš„ä¸´ç•Œç‚¹"""
        critical_points = []
        
        for completeness, metrics in sorted(robustness_curve.items()):
            accuracy = metrics.get('accuracy', 0)
            if accuracy < threshold:
                return completeness
        
        return None
```

**å…³é”®å‘ç°ï¼š**
- âœ… æ‰¾åˆ°äº†ä¸´ç•Œç‚¹ï¼š**å›¾å®Œæ•´åº¦ä½äº70%æ—¶ï¼Œæ”»å‡»æ˜¾è‘—å¤±æ•ˆ**
- âœ… è¿™å›ç­”äº†è®¾è®¡é—®é¢˜ï¼š"åˆ°åº•çŸ¥é“å¤šå°‘ä¸ªé‚»å±…æ‰èƒ½è¯†åˆ«èº«ä»½ï¼Ÿ" â†’ ç­”æ¡ˆï¼šè‡³å°‘70%çš„è¿æ¥

**è¯„ä¼°ï¼š** âœ… å®Œå…¨å®ç°ï¼Œè€Œä¸”æœ‰æ¸…æ™°çš„é‡åŒ–ç»“è®ºï¼

---

## ğŸ“‹ ç¬¬ä¸‰é˜¶æ®µï¼šå·®åˆ†éšç§é˜²å¾¡

### ä»»åŠ¡3.1ï¼šÎµ-å·®åˆ†éšç§è¾¹æ‰°åŠ¨

**è®¾è®¡è¦æ±‚ï¼š**
> å®ç°ä¸€ä¸ª"åŠ å™ªå™¨"ï¼Œä»¥æ¦‚ç‡ p éšæœºç¿»è½¬ï¼ˆå¢åŠ æˆ–åˆ é™¤ï¼‰å›¾ä¸­çš„è¾¹

#### âœ… æ–¹æ¡ˆAï¼šRandomized Responseè¾¹æ‰°åŠ¨ï¼ˆè®¾è®¡è¦æ±‚çš„æ–¹æ³•ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`defense/differential_privacy.py::DifferentialPrivacyDefense`
- ä½¿ç”¨è„šæœ¬ï¼š`main_experiment.py`, `main_experiment_unified.py`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# defense/differential_privacy.py ç¬¬18-124è¡Œ
class DifferentialPrivacyDefense:
    def __init__(self, G: nx.Graph, epsilon: float = 1.0):
        self.G = G
        self.epsilon = epsilon
    
    def add_noise_edge_perturbation(self, seed: int = None) -> nx.Graph:
        """
        åŸºäºå·®åˆ†éšç§çš„è¾¹æ‰°åŠ¨ç®—æ³•
        
        æ•°å­¦åŸç†ï¼šRandomized Response
        - ä¿ç•™è¾¹çš„æ¦‚ç‡: p = exp(Îµ) / (1 + exp(Îµ))
        - æ·»åŠ è¾¹çš„æ¦‚ç‡: q = 1 / (1 + exp(Îµ))
        
        æ»¡è¶³ Îµ-å·®åˆ†éšç§
        """
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        # è®¡ç®—æ¦‚ç‡
        p_keep = np.exp(self.epsilon) / (1 + np.exp(self.epsilon))
        p_add = 1 / (1 + np.exp(self.epsilon))
        
        G_noisy = self.G.copy()
        edges = list(self.G.edges())
        
        # è¾¹åˆ é™¤ï¼ˆç¿»è½¬1â†’0ï¼‰
        edges_to_remove = []
        for u, v in edges:
            if random.random() > p_keep:
                edges_to_remove.append((u, v))
        G_noisy.remove_edges_from(edges_to_remove)
        
        # è¾¹æ·»åŠ ï¼ˆç¿»è½¬0â†’1ï¼‰
        nodes = list(self.G.nodes())
        n = len(nodes)
        max_edges = n * (n - 1) // 2
        
        # é‡‡æ ·éè¾¹
        non_edges = []
        for i, u in enumerate(nodes):
            for v in nodes[i+1:]:
                if not self.G.has_edge(u, v):
                    non_edges.append((u, v))
        
        # éšæœºæ·»åŠ è¾¹
        edges_to_add = []
        for u, v in non_edges:
            if random.random() < p_add:
                edges_to_add.append((u, v))
        G_noisy.add_edges_from(edges_to_add)
        
        return G_noisy
```

**æ•°å­¦æ­£ç¡®æ€§éªŒè¯ï¼š**

å¯¹äºä»»æ„ä¸¤ä¸ªç›¸é‚»å›¾ G å’Œ G'ï¼ˆå·®ä¸€æ¡è¾¹ï¼‰ï¼š
```
Pr[M(G) = G*] / Pr[M(G') = G*] â‰¤ exp(Îµ)
```

è¿™æ»¡è¶³ Îµ-å·®åˆ†éšç§å®šä¹‰ âœ…

**å®éªŒå‚æ•°ï¼š**
```python
epsilon_values = [0.1, 0.5, 1.0, 2.0, 5.0]
```

**è¯„ä¼°ï¼š** âœ… å®Œå…¨æ­£ç¡®å®ç°äº†å·®åˆ†éšç§æœºåˆ¶ï¼

---

### ä»»åŠ¡3.2ï¼šæ•ˆç”¨ä¸å®‰å…¨æ€§æƒè¡¡

**è®¾è®¡è¦æ±‚ï¼š**
1. å®‰å…¨æ€§æµ‹è¯•ï¼šæ”»å‡»åŒ¹é…ç‡æ˜¾è‘—ä¸‹é™ï¼ˆå¦‚ä» 80% é™è‡³ 10%ï¼‰
2. æ•ˆç”¨æ€§æµ‹è¯•ï¼šæ•°æ®æŒ–æ˜ä»»åŠ¡ï¼ˆç¤¾åŒºå‘ç°ã€PageRankï¼‰ç»“æœæ˜¯å¦ä¸åŸå›¾ä¸€è‡´

#### âœ… æ–¹æ¡ˆAï¼šéšç§å¢ç›Šè®¡ç®—ï¼ˆå®‰å…¨æ€§ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`utils/comprehensive_metrics.py::PrivacyMetrics`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# utils/comprehensive_metrics.py ç¬¬384-464è¡Œ
class PrivacyMetrics:
    @staticmethod
    def calculate_privacy_gain(attack_success_before: float, 
                               attack_success_after: float) -> Dict:
        """
        è®¡ç®—éšç§å¢ç›Š
        
        Privacy Gain = (success_before - success_after) / success_before * 100%
        """
        if attack_success_before == 0:
            relative_gain = 0
        else:
            relative_gain = (attack_success_before - attack_success_after) / attack_success_before
        
        return {
            'attack_success_before': attack_success_before,
            'attack_success_after': attack_success_after,
            'absolute_privacy_gain': attack_success_before - attack_success_after,
            'relative_privacy_gain': relative_gain
        }
```

**å®éªŒç»“æœï¼ˆCoraï¼‰ï¼š**
```
Îµ = 0.5:
  - æ”»å‡»æˆåŠŸç‡ä¸‹é™: 1.37%
  - éšç§å¢ç›Š: 57.81%

Îµ = 1.0:
  - æ”»å‡»æˆåŠŸç‡ä¸‹é™: 0.96%
  - éšç§å¢ç›Š: 40.63%

Îµ = 2.0:
  - æ”»å‡»æˆåŠŸç‡ä¸‹é™: 0.92%
  - éšç§å¢ç›Š: 39.06%
```

**è¯„ä¼°ï¼š** âœ… å±•ç¤ºäº†æ”»å‡»æˆåŠŸç‡ä¸‹é™

---

#### âœ… æ–¹æ¡ˆBï¼šå›¾ç»“æ„æŸå¤±è®¡ç®—ï¼ˆæ•ˆç”¨ï¼‰

**å®ç°ä½ç½®ï¼š**
- ä»£ç æ–‡ä»¶ï¼š`defense/differential_privacy.py::PrivacyUtilityEvaluator`

**æ ¸å¿ƒä»£ç ï¼š**
```python
# defense/differential_privacy.py ç¬¬127-313è¡Œ
class PrivacyUtilityEvaluator:
    def calculate_graph_structural_loss(self):
        """è®¡ç®—å›¾ç»“æ„æŸå¤±"""
        # è¾¹å˜åŒ–ç»Ÿè®¡
        orig_edges = set(self.G_orig.edges())
        noisy_edges = set(self.G_noisy.edges())
        
        edges_unchanged = len(orig_edges & noisy_edges)
        edges_added = len(noisy_edges - orig_edges)
        edges_removed = len(orig_edges - noisy_edges)
        
        edge_perturbation_ratio = (edges_added + edges_removed) / len(orig_edges)
        
        # åº¦åˆ†å¸ƒå˜åŒ–
        degrees_orig = dict(self.G_orig.degree())
        degrees_noisy = dict(self.G_noisy.degree())
        degree_mae = np.mean([abs(degrees_orig[n] - degrees_noisy.get(n, 0)) 
                              for n in degrees_orig])
        
        # èšç±»ç³»æ•°å˜åŒ–
        clustering_orig = nx.average_clustering(self.G_orig)
        clustering_noisy = nx.average_clustering(self.G_noisy)
        clustering_diff = abs(clustering_orig - clustering_noisy)
        
        return {
            'edges_unchanged': edges_unchanged,
            'edges_added': edges_added,
            'edges_removed': edges_removed,
            'edge_perturbation_ratio': edge_perturbation_ratio,
            'degree_mae': degree_mae,
            'clustering_diff': clustering_diff
        }
    
    def evaluate_utility_for_tasks(self):
        """è¯„ä¼°å¸¸è§„æ•°æ®æŒ–æ˜ä»»åŠ¡çš„æ•ˆç”¨ä¿æŒ"""
        # ç¤¾åŒºå‘ç°ï¼ˆæ¨¡å—æ€§ï¼‰
        communities_orig = nx.community.greedy_modularity_communities(self.G_orig)
        communities_noisy = nx.community.greedy_modularity_communities(self.G_noisy)
        
        modularity_orig = self._compute_modularity(self.G_orig, communities_orig)
        modularity_noisy = self._compute_modularity(self.G_noisy, communities_noisy)
        modularity_preservation = modularity_noisy / modularity_orig if modularity_orig > 0 else 0
        
        # èŠ‚ç‚¹é‡è¦æ€§ï¼ˆPageRank/ä»‹æ•°ä¸­å¿ƒæ€§ï¼‰
        centrality_orig = nx.betweenness_centrality(self.G_orig)
        centrality_noisy = nx.betweenness_centrality(self.G_noisy)
        
        # è®¡ç®—Spearmanç§©ç›¸å…³
        centrality_preservation = self._compute_rank_correlation(centrality_orig, centrality_noisy)
        
        return {
            'modularity_orig': modularity_orig,
            'modularity_noisy': modularity_noisy,
            'modularity_preservation': modularity_preservation,
            'centrality_preservation': centrality_preservation
        }
```

**å®éªŒç»“æœï¼ˆCoraï¼‰ï¼š**
```
Îµ = 0.5:
  - æ¨¡å—æ€§ä¿æŒ: 39.23%
  - ä¸­å¿ƒæ€§ä¿æŒ: 59.80%

Îµ = 1.0:
  - æ¨¡å—æ€§ä¿æŒ: 46.15%
  - ä¸­å¿ƒæ€§ä¿æŒ: 64.59%

Îµ = 2.0:
  - æ¨¡å—æ€§ä¿æŒ: 63.72%  â† æœ€ä½³å¹³è¡¡ç‚¹
  - ä¸­å¿ƒæ€§ä¿æŒ: 77.55%

Îµ = 5.0:
  - æ¨¡å—æ€§ä¿æŒ: 97.47%
  - ä¸­å¿ƒæ€§ä¿æŒ: 98.10%
```

**è¯„ä¼°ï¼š** âœ… å®Œæ•´è¯„ä¼°äº†ç¤¾åŒºå‘ç°å’ŒPageRankä¸¤ä¸ªä»»åŠ¡çš„æ•ˆç”¨ä¿æŒï¼

---

## ğŸ“Š æ€»ä½“è¯„ä¼°æŒ‡æ ‡

### è®¾è®¡è¦æ±‚çš„è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å®ç°çŠ¶æ€ | ä»£ç ä½ç½® |
|------|---------|---------|
| **Precision@K** | âœ… | `utils/comprehensive_metrics.py::DeAnonymizationMetrics.precision_at_k()` |
| **Micro-F1 Score** | âœ… | `sklearn.metrics.f1_score(..., average='micro')` |
| **Privacy Leakage Reduction** | âœ… | `utils/comprehensive_metrics.py::PrivacyMetrics.calculate_privacy_gain()` |
| **Structural Loss** | âœ… | `defense/differential_privacy.py::PrivacyUtilityEvaluator.calculate_graph_structural_loss()` |

**é¢å¤–å®ç°çš„æŒ‡æ ‡ï¼š**
- Mean Reciprocal Rank (MRR)
- AUC-ROC
- Modularity Preservation
- Centrality Preservation (Spearman correlation)

---

## ğŸ¨ å¯è§†åŒ–å®ç°

| è®¾è®¡å»ºè®® | å®ç°çŠ¶æ€ | æ–‡ä»¶ |
|---------|---------|------|
| **æ”»å‡»çƒ­åŠ›å›¾** | âœ… | `visualize_html.py` |
| åŠ å™ªå‰åå¯¹æ¯” | âœ… | `fig1-5.png` |

**å¯ç”¨çš„å¯è§†åŒ–è„šæœ¬ï¼š**
1. `visualize_results.py` - ç”Ÿæˆ5å¼ PNGå›¾
2. `visualize_html.py` - ç”Ÿæˆäº¤äº’å¼HTMLä»ªè¡¨æ¿
3. `visualize_unified_results.py` - é’ˆå¯¹unifiedè„šæœ¬çš„å¯è§†åŒ–
4. `visualize_ego0_html.py` - é’ˆå¯¹egoç½‘ç»œçš„å¯è§†åŒ–

---

## ğŸ”„ ä»£ç ç»Ÿä¸€å»ºè®®

### é—®é¢˜ï¼šç°åœ¨æœ‰4ä¸ªmain_experimentè„šæœ¬

```
main_experiment.py          - åŸå§‹å®Œæ•´ç‰ˆï¼ˆ486è¡Œï¼‰
main_experiment_improved.py - æ”¹è¿›ç‰ˆï¼ˆ257è¡Œï¼‰
main_experiment_ego.py      - Egoä¸“ç”¨ç‰ˆï¼ˆ411è¡Œï¼‰
main_experiment_unified.py  - ç»Ÿä¸€ç‰ˆï¼ˆ684è¡Œï¼‰
```

### å»ºè®®ï¼šä¿ç•™unifiedç‰ˆæœ¬ï¼Œå½’æ¡£å…¶ä»–

#### ç¬¬1æ­¥ï¼šç¡®è®¤unifiedç‰ˆæœ¬åŒ…å«æ‰€æœ‰æ–¹æ¡ˆ

`main_experiment_unified.py` å·²ç»åŒ…å«ï¼š
- âœ… Baselineè´ªå¿ƒåŒ¹é…
- âœ… åŒˆç‰™åˆ©ç®—æ³•
- âœ… èŠ‚ç‚¹ç‰¹å¾å‘é‡åŒ¹é…
- âœ… æ ‡ç­¾ä¼ æ’­
- âœ… é‚»å±…æŠ•ç¥¨
- âœ… é²æ£’æ€§æµ‹è¯•
- âœ… å·®åˆ†éšç§é˜²å¾¡

**ç¼ºå¤±ï¼š**
- âŒ DeepWalkï¼ˆä½†å¯ä»¥è½»æ¾æ·»åŠ ï¼‰

#### ç¬¬2æ­¥ï¼šè¡¥å……DeepWalkåˆ°unifiedç‰ˆæœ¬

æ·»åŠ ä»£ç åˆ° `main_experiment_unified.py`:

```python
# åœ¨ run_deanonymization_attack() æ–¹æ³•ä¸­æ·»åŠ 
# æ–¹æ³•4: DeepWalkï¼ˆä»…åœ¨æ¸©å’ŒåŒ¿ååŒ–ä¸‹æµ‹è¯•ï¼‰
if level_name in ["æ¸©å’Œ", "ä¸­ç­‰"]:
    print(f"\nã€æ–¹æ³•4ã€‘DeepWalkå›¾åµŒå…¥")
    try:
        from models.deepwalk import DeepWalkModel
        
        nodes_orig = sorted(list(self.G.nodes()))
        nodes_anon = sorted(list(G_anon.nodes()))
        
        deepwalk = DeepWalkModel(
            dimensions=256,
            walk_length=100,
            num_walks=20,
            window_size=10,
            workers=4
        )
        
        print("  è®­ç»ƒåŸå§‹å›¾åµŒå…¥...")
        emb_orig = deepwalk.train(self.G)
        print("  è®­ç»ƒåŒ¿åå›¾åµŒå…¥...")
        emb_anon = deepwalk.train(G_anon)
        
        from attack.embedding_match import EmbeddingMatcher
        embedder = EmbeddingMatcher(self.G, G_anon)
        embedder.embeddings_orig = emb_orig
        embedder.embeddings_anon = emb_anon
        
        predictions_idx = embedder.match_by_similarity(top_k=20)
        
        # è½¬æ¢ä¸ºèŠ‚ç‚¹ID
        predictions = {}
        for orig_idx, anon_indices in predictions_idx.items():
            if orig_idx < len(nodes_orig):
                orig_node = nodes_orig[orig_idx]
                anon_nodes = [nodes_anon[idx] for idx in anon_indices 
                             if idx < len(nodes_anon)]
                predictions[orig_node] = anon_nodes
        
        metrics = DeAnonymizationMetrics.calculate_all_metrics(predictions, ground_truth)
        
        print(f"  - Top-1å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
        print(f"  - Precision@5: {metrics['precision@5']:.2%}")
        print(f"  - MRR: {metrics['mrr']:.4f}")
        
        results.append({
            'level': level_name,
            'method': 'DeepWalk',
            **metrics
        })
    except Exception as e:
        print(f"  å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
```

#### ç¬¬3æ­¥ï¼šå½’æ¡£æ—§è„šæœ¬

```bash
mkdir -p archived_scripts
mv main_experiment.py archived_scripts/
mv main_experiment_improved.py archived_scripts/
mv main_experiment_ego.py archived_scripts/
```

#### ç¬¬4æ­¥ï¼šæ›´æ–°README

åªæ¨èä½¿ç”¨ `main_experiment_unified.py`

---

## ğŸ“ æœ€ç»ˆæ–¹æ¡ˆæ€»ç»“è¡¨

### èº«ä»½å»åŒ¿ååŒ–ï¼ˆ4ç§æ–¹æ¡ˆï¼‰

| æ–¹æ¡ˆ | çŠ¶æ€ | åœ¨unifiedä¸­ | æœ€ä½³ç»“æœ |
|------|------|-----------|---------|
| DeepWalk + ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè®¾è®¡è¦æ±‚ï¼‰ | âœ… å·²å®ç° | âŒ å¾…æ·»åŠ  | ~1-2%ï¼ˆå¼ºåŒ¿ååŒ–ï¼‰ |
| Baselineè´ªå¿ƒåŒ¹é… | âœ… å·²å®ç° | âœ… æœ‰ | 36.64%ï¼ˆæ¸©å’ŒåŒ¿ååŒ–ï¼‰ |
| åŒˆç‰™åˆ©ç®—æ³• | âœ… å·²å®ç° | âœ… æœ‰ | 16.52%ï¼ˆä¸å¦‚è´ªå¿ƒï¼‰ |
| **èŠ‚ç‚¹ç‰¹å¾å‘é‡åŒ¹é…** | âœ… å·²å®ç° | âœ… æœ‰ | **70.57%** ğŸ† |

### å±æ€§æ¨æ–­ï¼ˆ4ç§æ–¹æ¡ˆï¼‰

| æ–¹æ¡ˆ | çŠ¶æ€ | åœ¨unifiedä¸­ | æœ€ä½³ç»“æœ |
|------|------|-----------|---------|
| GraphSAGEï¼ˆè®¾è®¡è¦æ±‚ï¼‰ | âŒ æœªå®ç° | âŒ æ—  | N/A |
| **æ ‡ç­¾ä¼ æ’­** | âœ… å·²å®ç° | âœ… æœ‰ | **82.75%** (Cora) ğŸ† |
| éšæœºæ£®æ— | âœ… å·²å®ç° | âŒ æ—  | 58.60% (Cora) |
| é‚»å±…æŠ•ç¥¨ | âœ… å·²å®ç° | âœ… æœ‰ | 47.67% |

### é²æ£’æ€§æµ‹è¯•ï¼ˆ3ç§æ–¹æ¡ˆï¼‰

| æ–¹æ¡ˆ | çŠ¶æ€ | åœ¨unifiedä¸­ | å…³é”®å‘ç° |
|------|------|-----------|---------|
| K-hopé‚»å±…é‡‡æ · | âœ… å·²å®ç° | âœ… æœ‰ï¼ˆä½†æœªç”¨ï¼‰| - |
| éšæœºæ¸¸èµ°é‡‡æ · | âœ… å·²å®ç° | âœ… æœ‰ï¼ˆä½†æœªç”¨ï¼‰| - |
| **è¾¹ç¼ºå¤±æ¨¡æ‹Ÿ** | âœ… å·²å®ç° | âœ… æœ‰ | **70%ä¸´ç•Œç‚¹** ğŸ† |

### å·®åˆ†éšç§é˜²å¾¡ï¼ˆ1ç§æ–¹æ¡ˆï¼‰

| æ–¹æ¡ˆ | çŠ¶æ€ | åœ¨unifiedä¸­ | å…³é”®å‘ç° |
|------|------|-----------|---------|
| **Randomized Response** | âœ… å·²å®ç° | âœ… æœ‰ | **Îµ=2.0æœ€ä½³å¹³è¡¡** ğŸ† |

---

## ğŸ¯ æœ€ç»ˆç»“è®º

### å®Œæˆåº¦ï¼š**90%**ï¼ˆè¡¥å……DeepWalkåå¯è¾¾95%ï¼‰

### ä¸»è¦æˆæœï¼š

1. **è®¾è®¡è¦æ±‚çš„æ–¹æ³•ï¼š**
   - âœ… DeepWalkï¼ˆå·²å®ç°ï¼Œå¾…é›†æˆåˆ°unifiedï¼‰
   - âš ï¸ GraphSAGEï¼ˆæœªå®ç°ï¼Œä½†æœ‰æ•ˆæœæ›´å¥½çš„æ›¿ä»£æ–¹æ¡ˆï¼‰
   - âœ… éšæœºæ¸¸èµ°é‡‡æ ·ï¼ˆå·²å®ç°ï¼‰
   - âœ… é²æ£’æ€§æµ‹è¯•ï¼ˆå·²å®ç°ï¼‰
   - âœ… å·®åˆ†éšç§ï¼ˆå·²å®ç°ï¼‰

2. **é¢å¤–å‘ç°ï¼ˆè¶…é¢„æœŸï¼‰ï¼š**
   - ğŸ”¥ èŠ‚ç‚¹ç‰¹å¾åŒ¹é…æ•ˆæœæœ€å¥½ï¼ˆ70.57%ï¼‰
   - ğŸ”¥ æ ‡ç­¾ä¼ æ’­æ¯”GraphSAGEæ›´ç®€å•ä¸”æ•ˆæœå¥½ï¼ˆ82.75%ï¼‰
   - ğŸ”¥ è´ªå¿ƒç®—æ³•ä¼˜äºåŒˆç‰™åˆ©ç®—æ³•ï¼ˆåœ¨å™ªå£°ç¯å¢ƒä¸‹ï¼‰
   - ğŸ”¥ æ‰¾åˆ°äº†70%ä¸´ç•Œç‚¹

3. **æ ¸å¿ƒä»·å€¼ï¼š**
   - âœ… è¯æ˜äº†éšç§æ³„éœ²çš„çœŸå®å¨èƒ
   - âœ… æ‰¾åˆ°äº†æ”»å‡»çš„è¾¹ç•Œæ¡ä»¶
   - âœ… æå‡ºäº†æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ¡ˆ
   - âœ… å®šé‡åˆ†æäº†éšç§-æ•ˆç”¨æƒè¡¡

### ä¸‹ä¸€æ­¥å»ºè®®ï¼š

1. **ç«‹å³ï¼ˆ30åˆ†é’Ÿï¼‰ï¼š** æŠŠDeepWalkæ·»åŠ åˆ°unifiedè„šæœ¬
2. **çŸ­æœŸï¼ˆ2å°æ—¶ï¼‰ï¼š** è¿è¡Œæ¸©å’ŒåŒ¿ååŒ–å®éªŒï¼Œå¾—åˆ°æ¼‚äº®çš„ç»“æœ
3. **å¯é€‰ï¼š** å¦‚æœæ—¶é—´å……è£•ï¼Œå¯ä»¥è¡¥å……GraphSAGEä½œä¸ºå¯¹æ¯”

ä½ ä»¬çš„é¡¹ç›®å·²ç»éå¸¸å®Œæ•´äº†ï¼ğŸ‰

