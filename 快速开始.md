# å¿«é€Ÿå¼€å§‹æŒ‡å— ğŸš€

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„**ç¤¾äº¤ç½‘ç»œå»åŒ¿ååŒ–æ”»å‡»**æ¡†æ¶ï¼Œç”¨äºè¯æ˜"å³ä¾¿æˆ‘ä¸è¯´è¯ï¼Œæˆ‘çš„æœ‹å‹ä¹Ÿä¼šæš´éœ²æˆ‘"è¿™ä¸€æ ¸å¿ƒè§‚ç‚¹ã€‚

## ä¸€ã€ç¯å¢ƒå‡†å¤‡ï¼ˆ5åˆ†é’Ÿï¼‰

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- 8GB+ RAMï¼ˆæ¨è16GBï¼‰
- Linux/macOS/Windows

### å®‰è£…ä¾èµ–

```bash
cd /home/honglianglu/hdd/deanony
pip install -r requirements.txt
```

æˆ–ä½¿ç”¨å¿«é€Ÿå®‰è£…è„šæœ¬ï¼š

```bash
chmod +x quick_start.sh
./quick_start.sh
```

## äºŒã€æ•°æ®å‡†å¤‡ï¼ˆæ–¹æ¡ˆé€‰æ‹©ï¼‰

### ğŸ¥‡ æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨GitHubæ•°æ®ï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰

#### æ­¥éª¤1ï¼šè·å–GitHub Token
1. è®¿é—® https://github.com/settings/tokens
2. ç‚¹å‡» "Generate new token (classic)"
3. å‹¾é€‰ `public_repo` æƒé™
4. å¤åˆ¶ç”Ÿæˆçš„token

#### æ­¥éª¤2ï¼šä¿®æ”¹çˆ¬è™«ä»£ç 
ç¼–è¾‘ `crawlers/github_crawler.py` çš„mainå‡½æ•°ï¼š

```python
# åœ¨main()å‡½æ•°ä¸­ä¿®æ”¹è¿™ä¸€è¡Œ
token = "ghp_ä½ çš„Token"  # æ›¿æ¢ä¸ºä½ çš„å®é™…token
```

#### æ­¥éª¤3ï¼šè¿è¡Œçˆ¬è™«

```bash
cd /home/honglianglu/hdd/deanony
python crawlers/github_crawler.py
```

é¢„è®¡æ—¶é—´ï¼š1-2å°æ—¶ï¼Œè·å–1000-5000ä¸ªèŠ‚ç‚¹

### ğŸ¥ˆ æ–¹æ¡ˆäºŒï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆæœ€å¿«ï¼‰

å¦‚æœä¸æƒ³çˆ¬è™«ï¼Œå¯ä»¥ç›´æ¥ç”Ÿæˆç¤ºä¾‹å›¾ï¼š

```python
import networkx as nx
import sys
sys.path.append('/home/honglianglu/hdd/deanony')

from preprocessing.graph_builder import GraphBuilder

# ç”Ÿæˆç¤ºä¾‹ç¤¾äº¤ç½‘ç»œï¼ˆBarabasi-Albertæ¨¡å‹ï¼‰
G = nx.barabasi_albert_graph(1000, 5)
print(f"ç¤ºä¾‹å›¾: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")

# ä¿å­˜
builder = GraphBuilder()
builder.graph = G
builder.save_graph(G, "data/processed/example_graph.gpickle")
```

### ğŸ¥‰ æ–¹æ¡ˆä¸‰ï¼šä¸‹è½½å…¬å¼€æ•°æ®é›†

è®¿é—® https://snap.stanford.edu/data/ ä¸‹è½½ï¼š
- `facebook_combined.txt.gz` (4,039èŠ‚ç‚¹)
- `github_stargazers.csv` (37,700èŠ‚ç‚¹)

## ä¸‰ã€è¿è¡Œå®Œæ•´å®éªŒï¼ˆ30åˆ†é’Ÿï¼‰

### æ–¹æ³•Aï¼šä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰

```bash
cd /home/honglianglu/hdd/deanony

# 1. æ„å»ºå›¾
python preprocessing/graph_builder.py

# 2. åŒ¿ååŒ–
python preprocessing/anonymizer.py

# 3. è¿è¡Œæ”»å‡»
python experiments/run_attack.py --method all --seed_ratio 0.05
```

### æ–¹æ³•Bï¼šä½¿ç”¨Pythonè„šæœ¬

åˆ›å»ºæ–‡ä»¶ `run_experiment.py`:

```python
import sys
sys.path.append('/home/honglianglu/hdd/deanony')

import pickle
from pathlib import Path
from preprocessing.graph_builder import GraphBuilder
from preprocessing.anonymizer import GraphAnonymizer
from experiments.run_attack import run_baseline_attack, run_deepwalk_attack

# 1. åŠ è½½æˆ–ç”Ÿæˆå›¾
import networkx as nx
G = nx.barabasi_albert_graph(500, 5)  # 500ä¸ªèŠ‚ç‚¹çš„ç¤ºä¾‹å›¾

# 2. æ„å»ºå’Œä¿å­˜
builder = GraphBuilder()
G = builder.compute_node_features(G)
builder.save_graph(G, Path("data/processed/example_graph.gpickle"))

# 3. åŒ¿ååŒ–
anonymizer = GraphAnonymizer(edge_retention_ratio=0.7)
G_anon, node_mapping = anonymizer.anonymize(G)
ground_truth = anonymizer.create_ground_truth(G, G_anon, node_mapping)
anonymizer.save_anonymized_data(G_anon, ground_truth, Path("data/anonymized"))

# 4. è¿è¡Œæ”»å‡»
print("\nè¿è¡ŒåŸºå‡†æ”»å‡»...")
results_baseline, _ = run_baseline_attack(G, G_anon, ground_truth)

print("\nè¿è¡ŒDeepWalkæ”»å‡»...")
results_deepwalk, _ = run_deepwalk_attack(G, G_anon, ground_truth, 
                                          use_alignment=True, seed_ratio=0.05)

print("\nå®éªŒå®Œæˆï¼")
print(f"åŸºå‡†æ–¹æ³•å‡†ç¡®ç‡: {results_baseline['accuracy']:.2%}")
print(f"DeepWalkå‡†ç¡®ç‡: {results_deepwalk['accuracy']:.2%}")
```

è¿è¡Œï¼š
```bash
python run_experiment.py
```

## å››ã€æŸ¥çœ‹ç»“æœ

### æ§åˆ¶å°è¾“å‡º

```
==================================================
å»åŒ¿ååŒ–æ”»å‡»è¯„ä¼°ç»“æœ
==================================================

å‡†ç¡®ç‡: 0.6523
ç²¾ç¡®ç‡: 0.6523
å¬å›ç‡: 0.6523
F1åˆ†æ•°: 0.6523

Top-Kå‡†ç¡®ç‡:
  Top-1: 0.6523
  Top-5: 0.8234
  Top-10: 0.8956

MRR (Mean Reciprocal Rank): 0.7234
==================================================
```

### ç»“æœæ–‡ä»¶

- `results/attack_results.json` - JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
- `results/attack_report.txt` - æ–‡æœ¬æ ¼å¼çš„æŠ¥å‘Š
- `results/graph_comparison.png` - åŸå§‹å›¾vsåŒ¿åå›¾å¯¹æ¯”
- `results/attack_comparison.png` - ä¸åŒæ–¹æ³•æ•ˆæœå¯¹æ¯”

## äº”ã€Jupyteræ¼”ç¤º

```bash
cd /home/honglianglu/hdd/deanony/notebooks

# æ–¹æ³•1ï¼šç›´æ¥è¿è¡ŒPythonè„šæœ¬
python demo.py

# æ–¹æ³•2ï¼šè½¬æ¢ä¸ºNotebook
pip install jupytext
jupytext --to notebook demo.py

# ç„¶åå¯åŠ¨Jupyter
jupyter notebook demo.ipynb
```

## å…­ã€å¸¸è§é—®é¢˜

### Q1: ModuleNotFoundError

```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
cd /home/honglianglu/hdd/deanony

# æˆ–åœ¨è„šæœ¬å¼€å¤´æ·»åŠ 
import sys
sys.path.insert(0, '/home/honglianglu/hdd/deanony')
```

### Q2: å†…å­˜ä¸è¶³

```python
# å‡å°‘èŠ‚ç‚¹æ•°é‡
G = nx.barabasi_albert_graph(500, 5)  # æ”¹ä¸º500

# æˆ–åœ¨DeepWalkä¸­å‡å°‘æ¸¸èµ°æ¬¡æ•°
model = DeepWalk(num_walks=5)  # é»˜è®¤10ï¼Œæ”¹ä¸º5
```

### Q3: GitHub APIé™æµ

```python
# å¢åŠ å»¶è¿Ÿ
crawler = GitHubCrawler(token=token, delay=1.0)  # å¢åŠ åˆ°1ç§’

# å‡å°‘çˆ¬å–æ•°é‡
data = crawler.crawl_network(max_users=500, max_depth=2)
```

### Q4: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶

```bash
# ç¡®ä¿ç›®å½•å­˜åœ¨
mkdir -p data/raw data/processed data/anonymized results models

# æ£€æŸ¥æ–‡ä»¶
ls -la data/processed/
ls -la data/anonymized/
```

## ä¸ƒã€æœ€å°å¯è¿è¡Œç¤ºä¾‹

å¦‚æœä½ åªæƒ³å¿«é€Ÿçœ‹åˆ°æ•ˆæœï¼Œè¿è¡Œè¿™ä¸ªæœ€å°ç¤ºä¾‹ï¼š

```python
# minimal_demo.py
import sys
sys.path.append('/home/honglianglu/hdd/deanony')

import networkx as nx
from preprocessing.anonymizer import GraphAnonymizer
from models.deepwalk import DeepWalk
from attack.embedding_match import EmbeddingMatcher
from utils.metrics import calculate_accuracy

# 1. ç”Ÿæˆå°å›¾ï¼ˆ100èŠ‚ç‚¹ï¼‰
G = nx.barabasi_albert_graph(100, 3)
print(f"åŸå§‹å›¾: {G.number_of_nodes()} èŠ‚ç‚¹")

# 2. åŒ¿ååŒ–
anonymizer = GraphAnonymizer(edge_retention_ratio=0.8)
G_anon, node_mapping = anonymizer.anonymize(G)
print(f"åŒ¿åå›¾: {G_anon.number_of_edges()} è¾¹")

# 3. DeepWalk
model_orig = DeepWalk(dimensions=32, walk_length=20, num_walks=5)
model_anon = DeepWalk(dimensions=32, walk_length=20, num_walks=5)
model_orig.fit(G)
model_anon.fit(G_anon)

# 4. åŒ¹é…
nodes_orig = sorted(G.nodes())
nodes_anon = sorted(G_anon.nodes())
emb_orig = model_orig.get_embeddings(nodes_orig)
emb_anon = model_anon.get_embeddings(nodes_anon)

matcher = EmbeddingMatcher()
sim_matrix = matcher.compute_similarity_matrix(emb_anon, emb_orig)
predictions = matcher.match_greedy(sim_matrix)

# 5. è¯„ä¼°
reverse_mapping = {v: k for k, v in node_mapping.items()}
gt = {i: nodes_orig.index(reverse_mapping[nodes_anon[i]]) 
      for i in range(len(nodes_anon))}
accuracy = calculate_accuracy(predictions, gt)

print(f"\nâœ… æ”»å‡»å‡†ç¡®ç‡: {accuracy:.2%}")
print(f"ğŸ’¡ ç»“è®º: å³ä½¿åˆ é™¤äº†{(1-0.8)*100:.0f}%çš„è¾¹ï¼Œä»èƒ½ä»¥{accuracy:.0%}çš„å‡†ç¡®ç‡è¯†åˆ«èŠ‚ç‚¹ï¼")
```

è¿è¡Œï¼š
```bash
python minimal_demo.py
```

é¢„è®¡è¾“å‡ºï¼š
```
åŸå§‹å›¾: 100 èŠ‚ç‚¹
åŒ¿åå›¾: 240 è¾¹
âœ… æ”»å‡»å‡†ç¡®ç‡: 65%
ğŸ’¡ ç»“è®º: å³ä½¿åˆ é™¤äº†20%çš„è¾¹ï¼Œä»èƒ½ä»¥65%çš„å‡†ç¡®ç‡è¯†åˆ«èŠ‚ç‚¹ï¼
```

## å…«ã€ä¸‹ä¸€æ­¥

### è¿›è¡Œæ·±å…¥å®éªŒ
1. æµ‹è¯•ä¸åŒç§å­æ¯”ä¾‹ï¼š1%, 3%, 5%, 10%
2. æµ‹è¯•ä¸åŒè¾¹ä¿ç•™ç‡ï¼š50%, 60%, 70%, 80%, 90%
3. å¯¹æ¯”ä¸åŒæ–¹æ³•ï¼šBaseline vs DeepWalk vs GraphSAGE

### æ’°å†™è®ºæ–‡
å‚è€ƒ `PROJECT_GUIDE.md` ä¸­çš„è®ºæ–‡ç»“æ„å»ºè®®

### åˆ¶ä½œæ¼”ç¤º
ä½¿ç”¨ `visualization/` ä¸­çš„å·¥å…·ç”Ÿæˆç²¾ç¾çš„å¯è§†åŒ–å›¾è¡¨

## ä¹ã€è·å–å¸®åŠ©

- ğŸ“– è¯¦ç»†æ–‡æ¡£ï¼š`README.md`, `USAGE.md`, `PROJECT_GUIDE.md`
- ğŸ› é‡åˆ°é—®é¢˜ï¼šæ£€æŸ¥ `PROJECT_SUMMARY.md` çš„"æ³¨æ„äº‹é¡¹"éƒ¨åˆ†
- ğŸ’¬ æŠ€æœ¯é—®é¢˜ï¼šæIssueæˆ–æŸ¥é˜…ä»£ç ä¸­çš„æ³¨é‡Š

---

ç¥å®éªŒé¡ºåˆ©ï¼ğŸ‰


